{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Group Project/Final\n",
    "## Kaggle Competition: Random Acts of Pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Prompt:\n",
    "People post pizza requests on Reddit\n",
    "Build 2-class classifier\n",
    "Classify whether post will get pizza\n",
    "Practice mining features from text\n",
    "\n",
    "Reference links:\n",
    "https://www.kaggle.com/c/random-acts-of-pizza\n",
    "http://cs.stanford.edu/~althoff/raop-dataset/\n",
    "\n",
    "Data Set:\n",
    "This training dataset contains a collection of 5671 textual requests for pizza from the Reddit community \"Random Acts of Pizza\" together with their outcome (successful/unsuccessful) and meta-data.\n",
    "\n",
    "We will split the dataset into:\n",
    "25% for development\n",
    "75% for training\n",
    "\n",
    "A separate dataset file was provided for testing purposes, of which we do not have the labels as to whether or not a pizza was received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgin/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/mgin/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "df_train = pd.read_json('train.json')\n",
    "df_test = pd.read_json('test.json')\n",
    "\n",
    "# drop the target column from the data and use it for the labels\n",
    "classification_column_name = 'requester_received_pizza'\n",
    "\n",
    "train_data = df_train.drop([classification_column_name], axis=1)\n",
    "train_labels = df_train[classification_column_name]\n",
    "\n",
    "# use twenty-five percent of the training data for a dev data set\n",
    "# note that we cannot use the test data set here, because we are not given their labels\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(train_data, train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be parsing the message body of each pizza request to utilize as features for our models, we will create term-frequency matricies of the text to use in our models as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decimal_to_percent(decimal):\n",
    "    return round(decimal * 100, 2)\n",
    "\n",
    "def basic_vectorizer():\n",
    "    ''' Construct term-frequency matrices for use in models '''\n",
    "    \n",
    "    # use just the text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data[text_column]\n",
    "    dev_text = dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tf_vect = CountVectorizer()\n",
    "    tf_train = tf_vect.fit_transform(train_text)\n",
    "    tf_dev = tf_vect.transform(dev_text)\n",
    "    \n",
    "    #output_file(\"basic_vocab.txt\", tf_vect.get_feature_names())\n",
    "    \n",
    "    return (tf_train, tf_dev)\n",
    "\n",
    "(tf_train, tf_dev) = basic_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train basic models, without tuning, for each candidate learning model:\n",
    "  Logistic Regression\n",
    "  Naive Bayes\n",
    "  Decision Tree\n",
    "  \n",
    "We will train and find the accuracies of each model.  This will give us a general idea of which learning models may be most successful and can build upon them from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, tf_train, train_labels, tf_dev, dev_labels):\n",
    "    ''' Train and score a model'''\n",
    "    clf = model\n",
    "    clf.fit(tf_train, train_labels)\n",
    "    \n",
    "    # return the accuracy and F1 scores\n",
    "    accuracy = clf.score(tf_dev, dev_labels) \n",
    "    predicted = clf.predict(tf_dev)\n",
    "    f1_score = metrics.f1_score(predicted, dev_labels, average=None)\n",
    "\n",
    "    return accuracy, f1_score\n",
    "\n",
    "def print_model_scores(model_type, accuracy, f1_score):\n",
    "    ''' Print the accuracy and f1 scores '''\n",
    "    \n",
    "    print 'The accuracy of {} model is {}%\\n'.format(model_type, decimal_to_percent(accuracy))\n",
    "    print 'The F1 scores are:\\nFalse: {}\\nTrue: {}\\n'.format(*[decimal_to_percent(score) for score in f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 69.7%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 81.02\n",
      "True: 25.0\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.39%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.08\n",
      "True: 28.99\n",
      "\n",
      "The accuracy of Decision Tree model is 65.54%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 77.43\n",
      "True: 27.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_lr = LogisticRegression()\n",
    "basic_nb = BernoulliNB()\n",
    "basic_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_lr, 'Logistic Regression'), (basic_nb, 'Naive Bayes'),\n",
    "                  (basic_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_train, train_labels, tf_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets attempt to build upon our basic models by introducing preprocessing algorithms for our word vocabulary vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_with_preprocessor(preprocessor_func):\n",
    "    ''' Construct term-frequency matrices for use in models '''\n",
    "    \n",
    "    # use just the text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data[text_column]\n",
    "    dev_text = dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tf_vect = CountVectorizer(preprocessor=preprocessor_func)\n",
    "    tf = tf_vect.fit(train_text)\n",
    "    \n",
    "    # make the matrices global variables for convenience?\n",
    "    tf_train_pp = tf.transform(train_text)\n",
    "    tf_dev_pp = tf.transform(dev_text)\n",
    "    \n",
    "    #output_file(\"porter.txt\", tf_vect.get_feature_names())\n",
    "    \n",
    "    return (tf_train_pp, tf_dev_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use porter-stemming algorithm to generalize words in the messages\n",
    "# https://tartarus.org/martin/PorterStemmer/def.txt\n",
    "def porter_stemming(s):\n",
    "    # create a new empty string, since s is the entire message not a single word\n",
    "    new_s = \"\"\n",
    "    \n",
    "    # make everything lowercase\n",
    "    s = s.lower()\n",
    "    # eliminate special characters\n",
    "    s = re.sub('[^A-Za-z0-9\\s]+', ' ', s)\n",
    "    # eliminate repeated numbers\n",
    "    s = re.sub('([0-9])[0-9]+', r'\\1', s)\n",
    "    \n",
    "    # iterate through each word in space delimited string\n",
    "    for w in s.split():\n",
    "        # calculate the measure, which is the number of vowel to consanant transitions\n",
    "        m_cnt = 0\n",
    "        if (re.search('^[^aeiou]*(([aeiou]+[^aeiou]+)+)[aeiou]*$', w)):\n",
    "            measure_match = re.match('^[^aeiou]*(([aeiou]+[^aeiou]+)+)[aeiou]*$', w)\n",
    "            # split on vowels to count number of transitions\n",
    "            consanant_groups = re.split('[aeiou]+', measure_match.group(1))\n",
    "            m_cnt = len(consanant_groups) - 1\n",
    "        \n",
    "        # Step 1a of porter stemming\n",
    "        if re.search('sses$', w):\n",
    "            w = re.sub('sses$', 'ss', w)\n",
    "        elif re.search('ies$', w):\n",
    "            w = re.sub('ies$', 'i', w)\n",
    "        elif re.search('ss$', w):\n",
    "            w = re.sub('ss$', 'i', w)\n",
    "        elif re.search('s$', w):\n",
    "            w = re.sub('s$', '', w)\n",
    "        # Step 1b\n",
    "        # Porter-Stemming says this should be m_cnt > 0, but doesn't\n",
    "        # even match their own examples, tweaked to 1 and got slightly better performance\n",
    "        if (m_cnt > 1 and re.search('eed$', w)):\n",
    "            w = re.sub('eed$', 'ee', w)\n",
    "        elif (re.search('.*[aeiou].*(ed|ing)$', w)):\n",
    "            w = re.sub('(ed|ing)$', '', w)\n",
    "            # if the second or third rule of 1b is successful, we also\n",
    "            if (re.search('(at|bl|iz)$', w)):\n",
    "                w += 'e'\n",
    "            # ends in double consanant, but no l s or z\n",
    "            elif (re.search('.*([^aeiou])([^aeiou])$', w)) :\n",
    "                m = re.match('(.*)([^aeiou])([^aeiou])$', w)\n",
    "                if (m.group(3) == m.group(2) and\n",
    "                    m.group(3) != 'l' and\n",
    "                    m.group(3) != 's' and\n",
    "                    m.group(3) != 'z') :\n",
    "                    w = m.group(1) + m.group(2)\n",
    "            # measure at least one and ends in cvc\n",
    "            # but second c is not W,X,Y\n",
    "            elif (m_cnt == 1 and re.search('^.*[^aeiou][aeiou][^aeiouwxy]$', w)) :\n",
    "                w = re.sub('[^aeiouwxy]$', 'e', w)\n",
    "        # Step 1c\n",
    "        if (re.search('.*[aeiou].*y$', w)) :\n",
    "            w = re.sub('y$', 'i', w)\n",
    "        # Step 2\n",
    "        if (m_cnt > 0) :\n",
    "            if (re.search('ational$', w)) :\n",
    "                w = re.sub('ational$', 'ate', w)\n",
    "            elif (re.search('tional$', w)) :\n",
    "                w = re.sub('tional$', 'tion', w)\n",
    "            elif (re.search('enci$', w)) :\n",
    "                w = re.sub('enci$', 'ence', w)\n",
    "            elif (re.search('anci$', w)) :\n",
    "                w = re.sub('anci$', 'ance', w)\n",
    "            elif (re.search('izer$', w)) :\n",
    "                w = re.sub('izer$', 'ize', w)\n",
    "            elif (re.search('abli$', w)) :\n",
    "                w = re.sub('abli$', 'able', w)\n",
    "            elif (re.search('alli$', w)) :\n",
    "                w = re.sub('alli$', 'al', w)\n",
    "            elif (re.search('entli$', w)) :\n",
    "                w = re.sub('entli$', 'ent', w)\n",
    "            elif (re.search('eli$', w)) :\n",
    "                w = re.sub('eli$', 'e', w)\n",
    "            elif (re.search('ousli$', w)) :\n",
    "                w = re.sub('ousli$', 'ous', w)\n",
    "            elif (re.search('ization$', w)) :\n",
    "                w = re.sub('ization$', 'ize', w)\n",
    "            elif (re.search('(ation|ator)$', w)) :\n",
    "                w = re.sub('(ation|ator)$', 'ate', w)\n",
    "            elif (re.search('alism$', w)) :\n",
    "                w = re.sub('alism$', 'al', w)\n",
    "            elif (re.search('iveness$', w)) :\n",
    "                w = re.sub('iveness$', 'ive', w)\n",
    "            elif (re.search('fulness$', w)) :\n",
    "                w = re.sub('fulness$', 'ful', w)\n",
    "            elif (re.search('ousness$', w)) :\n",
    "                w = re.sub('ousness$', 'ous', w)\n",
    "            elif (re.search('aliti$', w)) :\n",
    "                w = re.sub('aliti$', 'al', w)\n",
    "            elif (re.search('iviti$', w)) :\n",
    "                w = re.sub('iviti$', 'ive', w)\n",
    "            elif (re.search('biliti$', w)) :\n",
    "                w = re.sub('biliti$', 'ble', w)\n",
    "        # Step 3\n",
    "        if (m_cnt > 0) :\n",
    "            if (re.search('icate$', w)) :\n",
    "                w = re.sub('icate$', 'ic', w)\n",
    "            elif (re.search('ative$', w)) :\n",
    "                w = re.sub('ative$', '', w)\n",
    "            elif (re.search('alize$', w)) :\n",
    "                w = re.sub('alize$', 'al', w)\n",
    "            elif (re.search('iciti$', w)) :\n",
    "                w = re.sub('iciti$', 'ic', w)\n",
    "            elif (re.search('ical$', w)) :\n",
    "                w = re.sub('ical$', 'ic', w)\n",
    "            elif (re.search('ful$', w)) :\n",
    "                w = re.sub('ful$', '', w)\n",
    "            elif (re.search('ness$', w)) :\n",
    "                w = re.sub('ness$', '', w)\n",
    "        # Step 4\n",
    "        if (m_cnt > 1) :\n",
    "            if (re.search('al$', w)) :\n",
    "                w = re.sub('al$', '', w)\n",
    "            elif (re.search('ance$', w)) :\n",
    "                w = re.sub('ance$', '', w)\n",
    "            elif (re.search('ence$', w)) :\n",
    "                w = re.sub('ence$', '', w)\n",
    "            elif (re.search('er$', w)) :\n",
    "                w = re.sub('er$', '', w)\n",
    "            elif (re.search('ic$', w)) :\n",
    "                w = re.sub('ic$', '', w)\n",
    "            elif (re.search('able$', w)) :\n",
    "                w = re.sub('able$', '', w)\n",
    "            elif (re.search('ible$', w)) :\n",
    "                w = re.sub('ible$', '', w)\n",
    "            elif (re.search('ant$', w)) :\n",
    "                w = re.sub('ant$', '', w)\n",
    "            elif (re.search('ement$', w)) :\n",
    "                w = re.sub('ement$', '', w)\n",
    "            elif (re.search('ent$', w)) :\n",
    "                w = re.sub('ent$', '', w)\n",
    "            elif (m_cnt > 1 and re.search('[s|t]ion$', w)) :\n",
    "                w = re.sub('[s|t]ion$', '', w)\n",
    "            elif (re.search('ou$', w)) :\n",
    "                w = re.sub('ou$', '', w)\n",
    "            elif (re.search('ism$', w)) :\n",
    "                w = re.sub('ism$', '', w)\n",
    "            elif (re.search('ate$', w)) :\n",
    "                w = re.sub('ate$', '', w)\n",
    "            elif (re.search('iti$', w)) :\n",
    "                w = re.sub('iti$', '', w)\n",
    "            elif (re.search('ous$', w)) :\n",
    "                w = re.sub('ous$', '', w)\n",
    "            elif (re.search('ive$', w)) :\n",
    "                w = re.sub('ive$', '', w)\n",
    "            elif (re.search('ize$', w)) :\n",
    "                w = re.sub('ize$', '', w)\n",
    "        # Step 5a\n",
    "        if (m_cnt > 1 and re.search('e$', w)) :\n",
    "            w = re.sub('e$', '', w)\n",
    "        # measure at least one and ends in cvc\n",
    "        # but second c is not W,X,Y\n",
    "        elif (m_cnt == 1 and not re.search('^[^aeiou][aeiou][^aeiouwxy]e$', w)) :\n",
    "            w = re.sub('e$', '', w)\n",
    "        # Step 5b\n",
    "        if (m_cnt > 1 and re.search('.*ll$', w)):\n",
    "            w = re.sub('l$', '', w)\n",
    "        # end of porter stemming\n",
    "        # attach the word back to the new string\n",
    "        new_s += (\" \" + w)\n",
    "    return new_s\n",
    "\n",
    "def output_file(output_name, output_list):\n",
    "    with open(output_name, 'w') as output:\n",
    "        for i in output_list:\n",
    "            output.write(i.encode('UTF-8') + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopword_preprocessor(s):\n",
    "    # make everything lowercase\n",
    "    s = s.lower()\n",
    "    # eliminate repeated numbers\n",
    "    s = re.sub('([0-9])[0-9]+', r'\\1', s)\n",
    "    # eliminate special characters\n",
    "    s = re.sub('[^A-Za-z0-9\\s\\-]+', ' ', s)\n",
    "    # add a space between consecutive numbers/alpha\n",
    "    s = re.sub('([0-9])([a-z])', '\\1 \\2', s)\n",
    "    s = re.sub('([a-z])([0-9])', '\\1 \\2', s)\n",
    "    \n",
    "    # remove \"stop\" words which bear no significant meaning in most contexts\n",
    "    s = re.sub(' ?the ', r' ', s)\n",
    "    s = re.sub(' ?who ', r' ', s)\n",
    "    s = re.sub(' ?what ', r' ', s)\n",
    "    s = re.sub(' ?them ', r' ', s)\n",
    "    s = re.sub(' ?my ', r' ', s)\n",
    "    s = re.sub(' ?our ', r' ', s)\n",
    "    s = re.sub(' ?this ', r' ', s)\n",
    "    s = re.sub(' ?that ', r' ', s)\n",
    "    s = re.sub(' ?which ', r' ', s)\n",
    "    s = re.sub(' ?why ', r' ', s)\n",
    "    s = re.sub(' ?me ', r' ', s)\n",
    "    #s = re.sub(' ?i ', r' ', s)\n",
    "    #s = re.sub(' ?us ', r' ', s)\n",
    "    #s = re.sub(' ?you ', r' ', s)\n",
    "    s = re.sub(' ?they ', r' ', s)\n",
    "    s = re.sub(' ?where ', r' ', s)\n",
    "    s = re.sub(' ?and ', r' ', s)\n",
    "    s = re.sub(' ?for ', r' ', s)\n",
    "    s = re.sub(' ?his ', r' ', s)\n",
    "    s = re.sub(' ?her ', r' ', s)\n",
    "    s = re.sub(' ?to ', r' ', s)\n",
    "    #s = re.sub(' ?of ', r' ', s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain new basic models, this time we use an updated vocabulary based on our vectorizer preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 70.99%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 81.97\n",
      "True: 25.82\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.58%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.32\n",
      "True: 27.71\n",
      "\n",
      "The accuracy of Decision Tree model is 67.43%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 79.08\n",
      "True: 26.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(tf_pp_train, tf_pp_dev) = vectorize_with_preprocessor(stopword_preprocessor)\n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_pp_lr = LogisticRegression()\n",
    "basic_pp_nb = BernoulliNB()\n",
    "basic_pp_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_pp_lr, 'Logistic Regression'), (basic_pp_nb, 'Naive Bayes'),\n",
    "                  (basic_pp_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_pp_train, train_labels,\n",
    "                                                  tf_pp_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the stopword preprocessor, the accuracy of for our models improved:\n",
    "Logistic Regression model from 69.7% to 70.99%\n",
    "Naive Bayes model from 71.39% to 71.58%\n",
    "Decision Tree model from 64.75% to 66.53%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 70.99%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 81.97\n",
      "True: 25.82\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.58%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.32\n",
      "True: 27.71\n",
      "\n",
      "The accuracy of Decision Tree model is 65.35%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 77.3\n",
      "True: 26.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize textual data with TF-IDF transformation\n",
    "def tfidf_vectorizer(preprocessor_func):\n",
    "    # use just the text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data[text_column]\n",
    "    dev_text = dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tfidf_vect = TfidfVectorizer(preprocessor=preprocessor_func)\n",
    "    tfidf = tfidf_vect.fit(train_text)\n",
    "    \n",
    "    # make the matrices global variables for convenience?\n",
    "    tfidf_train = tf.transform(train_text)\n",
    "    tfidf_dev = tf.transform(dev_text)\n",
    "    \n",
    "    return (tfidf_train, tfidf_dev)\n",
    "\n",
    "(tfidf_train, tfidf_dev) = vectorize_with_preprocessor(stopword_preprocessor)\n",
    "\n",
    "basic_tfidf_lr = LogisticRegression()\n",
    "basic_tfidf_nb = BernoulliNB()\n",
    "basic_tfidf_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_tfidf_lr, 'Logistic Regression'),\n",
    "                          (basic_tfidf_nb, 'Naive Bayes'),\n",
    "                          (basic_tfidf_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model,\n",
    "                                                  tfidf_train, train_labels,\n",
    "                                                  tfidf_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, Naive Bayes appears to be performing better than the Logistic Regression and Decision Tree models, although only about 2% better over the Logistic Regression model.\n",
    "\n",
    "We will attempt to tune our default modeling by gridsearching over various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'C': 0.001}\n",
      "The accuracy of Tuned Logistic Regression model is 74.55%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.42\n",
      "True: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generic grid search which can field different models and hyperparameters\n",
    "# and output the best results\n",
    "def gridsearch_model(model, parameters, tf_dev, dev_labels): \n",
    "    gridsearch = GridSearchCV(estimator=model,\n",
    "                              param_grid=parameters)\n",
    "    gridsearch.fit(tf_dev, dev_labels)\n",
    "    \n",
    "    print \"Best parameters:\"\n",
    "    print gridsearch.best_params_\n",
    "    \n",
    "# c_values for logistic regression\n",
    "c_values = {'C': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0,\n",
    "                  5.0, 10.0, 15.0, 20.0, 25.0, 50.0, 75.0, 100.0]}\n",
    "\n",
    "# logistic regression tuning\n",
    "#lr_basic = LogisticRegression()\n",
    "#lr_basic.fit(tf_pp_train, train_labels)\n",
    "gridsearch_model(basic_pp_lr, c_values, tf_pp_dev, dev_labels)\n",
    "\n",
    "# use optimal parameters\n",
    "tuned_lr = LogisticRegression(C=0.001)\n",
    "tuned_lr.fit(tf_pp_train, train_labels)\n",
    "lr_accuracy = tuned_lr.score(tf_pp_dev, dev_labels) \n",
    "lr_predicted = tuned_lr.predict(tf_pp_dev)\n",
    "lr_f1_score = metrics.f1_score(lr_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Logistic Regression\", lr_accuracy, lr_f1_score)\n",
    "#print \"Accuracy: \", lr_accuracy\n",
    "#print \"F1 Score: \", lr_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'alpha': 0.9}\n",
      "The accuracy of Tuned Naive Bayes model is 71.39%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.06\n",
      "True: 29.34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# naive bayes tuning\n",
    "#nb_basic = BernoulliNB()\n",
    "#nb_basic.fit(tx_train, train_labels)\n",
    "\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0,]}\n",
    "gridsearch_model(basic_pp_nb, params, tf_pp_dev, dev_labels)\n",
    "\n",
    "tuned_nb = BernoulliNB(alpha=0.9)\n",
    "tuned_nb.fit(tf_pp_train, train_labels)\n",
    "nb_accuracy = tuned_nb.score(tf_pp_dev, dev_labels)\n",
    "nb_predicted = tuned_nb.predict(tf_pp_dev)\n",
    "nb_f1_score = metrics.f1_score(nb_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Naive Bayes\", nb_accuracy, nb_f1_score)\n",
    "#print \"Accuracy: \", nb_accuracy\n",
    "#print \"F1 Score: \", nb_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the tuned hyper parameters produce better overall accuracy, in the case of logistic regression, we are getting 0% results for \"True\" or successful Pizza Requests.  It possible there is a class imbalance in our training data and the model is merely always guessing False/unsuccessful pizza requests to obtain a higher accuracy.\n",
    "\n",
    "If there is a class imbalance in our training data, it may be difficult for the model to distinguish between sucessful and unsucessful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2293\n",
      "1 737\n"
     ]
    }
   ],
   "source": [
    "# doing some exploration to find out if there is a class imbalance in our training data\n",
    "def class_counts():\n",
    "    counts = [0, 0]\n",
    "    for i in train_labels:\n",
    "        counts[i] += 1\n",
    "    \n",
    "    for i in range(len(counts)):\n",
    "        print i, counts[i]\n",
    "        \n",
    "class_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Weighed Logistic Regression model is 70.79%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 81.78\n",
      "True: 26.43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There is some class imbalance, so lets try weighting\n",
    "weight_dict = {0: 0.35, 1: .65}\n",
    "lr_weight = LogisticRegression(C=0.001, class_weight=weight_dict)\n",
    "lr_weight.fit(tf_pp_train, train_labels)\n",
    "lrw_accuracy = lr_weight.score(tf_pp_dev, dev_labels) \n",
    "lrw_predicted = lr_weight.predict(tf_pp_dev)\n",
    "lrw_f1_score = metrics.f1_score(lrw_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Weighed Logistic Regression\", lrw_accuracy, lrw_f1_score)\n",
    "#print \"Accuracy: \", lrw_accuracy\n",
    "#print \"F1 Score: \", lrw_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

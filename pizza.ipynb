{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Group Project/Final\n",
    "## Kaggle Competition: Random Acts of Pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Members:<br>\n",
    "Daniel Elkin<br>\n",
    "Mark Gin\n",
    "\n",
    "Project Prompt:<br>\n",
    "People post pizza requests on Reddit<br>\n",
    "Build 2-class classifier<br>\n",
    "Classify whether post will get pizza<br>\n",
    "Practice mining features from text<br>\n",
    "\n",
    "Reference links:\n",
    " - https://www.kaggle.com/c/random-acts-of-pizza\n",
    " - http://cs.stanford.edu/~althoff/raop-dataset/\n",
    "\n",
    "Data Set:<br>\n",
    "This training dataset contains a collection of 5671 textual requests for pizza from the Reddit community \"Random Acts of Pizza\" together with their outcome (successful/unsuccessful) and meta-data.\n",
    "\n",
    "We will split the dataset into:\n",
    " - 25% for development\n",
    " - 75% for training\n",
    "\n",
    "A separate dataset file was provided for testing purposes, of which we do not have the labels as to whether or not a pizza was received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgin/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/mgin/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "df_train = pd.read_json('train.json')\n",
    "df_test = pd.read_json('test.json')\n",
    "\n",
    "# drop the target column from the data and use it for the labels\n",
    "classification_column_name = 'requester_received_pizza'\n",
    "\n",
    "train_data = df_train.drop([classification_column_name], axis=1)\n",
    "train_labels = df_train[classification_column_name]\n",
    "\n",
    "# use twenty-five percent of the training data for a dev data set\n",
    "# note that we cannot use the test data set here, because we are not given their labels\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(train_data, train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be parsing the message body of each pizza request to utilize as features for our models, we will create term-frequency matricies of the text to use in our models as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# debug function used to print the vocabularies into a text file\n",
    "def output_file(output_name, output_list):\n",
    "    with open(output_name, 'w') as output:\n",
    "        for i in output_list:\n",
    "            output.write(i.encode('UTF-8') + \"\\n\")\n",
    "\n",
    "def decimal_to_percent(decimal):\n",
    "    return round(decimal * 100, 2)\n",
    "\n",
    "def basic_vectorizer():\n",
    "    ''' Construct term-frequency matrices for use in models '''\n",
    "    \n",
    "    # use title and text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data['request_title'] + train_data[text_column]\n",
    "    dev_text = dev_data['request_title'] + dev_data[text_column]\n",
    "#    train_text = train_data[text_column]\n",
    "#    dev_text = dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tf_vect = CountVectorizer()\n",
    "    tf_train = tf_vect.fit_transform(train_text)\n",
    "    tf_dev = tf_vect.transform(dev_text)\n",
    "    \n",
    "    #output_file(\"basic_vocab.txt\", tf_vect.get_feature_names())\n",
    "    \n",
    "    return (tf_train, tf_dev)\n",
    "\n",
    "(tf_train, tf_dev) = basic_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train basic models, without tuning, for each candidate learning model:\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Decision Tree\n",
    "  \n",
    "We will train and find the accuracies of each model.  This will give us a general idea of which learning models may be most successful and can build upon them from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, tf_train, train_labels, tf_dev, dev_labels):\n",
    "    ''' Train and score a model'''\n",
    "    clf = model\n",
    "    clf.fit(tf_train, train_labels)\n",
    "    \n",
    "    # return the accuracy and F1 scores\n",
    "    accuracy = clf.score(tf_dev, dev_labels) \n",
    "    predicted = clf.predict(tf_dev)\n",
    "    f1_score = metrics.f1_score(predicted, dev_labels, average=None)\n",
    "\n",
    "    return accuracy, f1_score\n",
    "\n",
    "def print_model_scores(model_type, accuracy, f1_score):\n",
    "    ''' Print the accuracy and f1 scores '''\n",
    "    \n",
    "    print 'The accuracy of {} model is {}%\\n'.format(model_type, decimal_to_percent(accuracy))\n",
    "    print 'The F1 scores are:\\nFalse: {}\\nTrue: {}\\n'.format(*[decimal_to_percent(score) for score in f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 69.21%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 80.48\n",
      "True: 27.17\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.98%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.8\n",
      "True: 24.53\n",
      "\n",
      "The accuracy of Decision Tree model is 67.82%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 79.02\n",
      "True: 31.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_lr = LogisticRegression()\n",
    "basic_nb = BernoulliNB()\n",
    "basic_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_lr, 'Logistic Regression'), (basic_nb, 'Naive Bayes'),\n",
    "                  (basic_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_train, train_labels, tf_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to build upon our basic models by introducing preprocessing algorithms for our word vocabulary vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_with_preprocessor(preprocessor_func, ngram=(1,1), mindf=1):\n",
    "    ''' Construct term-frequency matrices for use in models '''\n",
    "    \n",
    "    # use title and text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data['request_title'] + train_data[text_column]\n",
    "    dev_text = dev_data['request_title'] + dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tf_vect = CountVectorizer(preprocessor=preprocessor_func,\n",
    "                              ngram_range=ngram,\n",
    "                              min_df=mindf)\n",
    "    tf = tf_vect.fit(train_text)\n",
    "    \n",
    "    # make the matrices global variables for convenience?\n",
    "    tf_train_pp = tf.transform(train_text)\n",
    "    tf_dev_pp = tf.transform(dev_text)\n",
    "    \n",
    "    #output_file(\"porter.txt\", tf_vect.get_feature_names())\n",
    "    #output_file(\"composite.txt\", tf_vect.get_feature_names())\n",
    "    \n",
    "    return (tf_train_pp, tf_dev_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use porter-stemming algorithm to generalize words in the messages\n",
    "# https://tartarus.org/martin/PorterStemmer/def.txt\n",
    "def porter_stemming(s):\n",
    "    # create a new empty string, since s is the entire message not a single word\n",
    "    new_s = \"\"\n",
    "    \n",
    "    # iterate through each word in space delimited string\n",
    "    for w in s.split():\n",
    "        # calculate the measure, which is the number of vowel to consanant transitions\n",
    "        m_cnt = 0\n",
    "        if (re.search('^[^aeiou]*(([aeiou]+[^aeiou]+)+)[aeiou]*$', w)):\n",
    "            measure_match = re.match('^[^aeiou]*(([aeiou]+[^aeiou]+)+)[aeiou]*$', w)\n",
    "            # split on vowels to count number of transitions\n",
    "            consanant_groups = re.split('[aeiou]+', measure_match.group(1))\n",
    "            m_cnt = len(consanant_groups) - 1\n",
    "        \n",
    "        # Step 1a of porter stemming\n",
    "        if re.search('sses$', w):\n",
    "            w = re.sub('sses$', 'ss', w)\n",
    "        elif re.search('ies$', w):\n",
    "            w = re.sub('ies$', 'i', w)\n",
    "        elif re.search('ss$', w):\n",
    "            w = re.sub('ss$', 'i', w)\n",
    "        elif re.search('s$', w):\n",
    "            w = re.sub('s$', '', w)\n",
    "        # Step 1b\n",
    "        # Porter-Stemming says this should be m_cnt > 0, but doesn't\n",
    "        # even match their own examples, tweaked to 1 and got slightly better performance\n",
    "#        if (m_cnt > 1 and re.search('eed$', w)):\n",
    "#            w = re.sub('eed$', 'ee', w)\n",
    "#        elif (re.search('.*[aeiou].*(ed|ing)$', w)):\n",
    "#            w = re.sub('(ed|ing)$', '', w)\n",
    "#            # if the second or third rule of 1b is successful, we also\n",
    "#            if (re.search('(at|bl|iz)$', w)):\n",
    "#                w += 'e'\n",
    "#            # ends in double consanant, but no l s or z\n",
    "#            elif (re.search('.*([^aeiou])([^aeiou])$', w)) :\n",
    "#                m = re.match('(.*)([^aeiou])([^aeiou])$', w)\n",
    "#                if (m.group(3) == m.group(2) and\n",
    "#                    m.group(3) != 'l' and\n",
    "#                    m.group(3) != 's' and\n",
    "#                    m.group(3) != 'z') :\n",
    "#                    w = m.group(1) + m.group(2)\n",
    "#            # measure at least one and ends in cvc\n",
    "#            # but second c is not W,X,Y\n",
    "#            elif (m_cnt == 1 and re.search('^.*[^aeiou][aeiou][^aeiouwxy]$', w)) :\n",
    "#                w = re.sub('[^aeiouwxy]$', 'e', w)\n",
    "        # Step 1c\n",
    "        if (re.search('.*[aeiou].*y$', w)) :\n",
    "            w = re.sub('y$', 'i', w)\n",
    "        # Step 2\n",
    "        if (m_cnt > 0) :\n",
    "            if (re.search('ational$', w)) :\n",
    "                w = re.sub('ational$', 'ate', w)\n",
    "            elif (re.search('tional$', w)) :\n",
    "                w = re.sub('tional$', 'tion', w)\n",
    "            elif (re.search('enci$', w)) :\n",
    "                w = re.sub('enci$', 'ence', w)\n",
    "            elif (re.search('anci$', w)) :\n",
    "                w = re.sub('anci$', 'ance', w)\n",
    "            elif (re.search('izer$', w)) :\n",
    "                w = re.sub('izer$', 'ize', w)\n",
    "            elif (re.search('abli$', w)) :\n",
    "                w = re.sub('abli$', 'able', w)\n",
    "            elif (re.search('alli$', w)) :\n",
    "                w = re.sub('alli$', 'al', w)\n",
    "            elif (re.search('entli$', w)) :\n",
    "                w = re.sub('entli$', 'ent', w)\n",
    "            elif (re.search('eli$', w)) :\n",
    "                w = re.sub('eli$', 'e', w)\n",
    "            elif (re.search('ousli$', w)) :\n",
    "                w = re.sub('ousli$', 'ous', w)\n",
    "            elif (re.search('ization$', w)) :\n",
    "                w = re.sub('ization$', 'ize', w)\n",
    "            elif (re.search('(ation|ator)$', w)) :\n",
    "                w = re.sub('(ation|ator)$', 'ate', w)\n",
    "            elif (re.search('alism$', w)) :\n",
    "                w = re.sub('alism$', 'al', w)\n",
    "            elif (re.search('iveness$', w)) :\n",
    "                w = re.sub('iveness$', 'ive', w)\n",
    "            elif (re.search('fulness$', w)) :\n",
    "                w = re.sub('fulness$', 'ful', w)\n",
    "            elif (re.search('ousness$', w)) :\n",
    "                w = re.sub('ousness$', 'ous', w)\n",
    "            elif (re.search('aliti$', w)) :\n",
    "                w = re.sub('aliti$', 'al', w)\n",
    "            elif (re.search('iviti$', w)) :\n",
    "                w = re.sub('iviti$', 'ive', w)\n",
    "            elif (re.search('biliti$', w)) :\n",
    "                w = re.sub('biliti$', 'ble', w)\n",
    "        # Step 3\n",
    "        if (m_cnt > 0) :\n",
    "            if (re.search('icate$', w)) :\n",
    "                w = re.sub('icate$', 'ic', w)\n",
    "            elif (re.search('ative$', w)) :\n",
    "                w = re.sub('ative$', '', w)\n",
    "            elif (re.search('alize$', w)) :\n",
    "                w = re.sub('alize$', 'al', w)\n",
    "            elif (re.search('iciti$', w)) :\n",
    "                w = re.sub('iciti$', 'ic', w)\n",
    "            elif (re.search('ical$', w)) :\n",
    "                w = re.sub('ical$', 'ic', w)\n",
    "            elif (re.search('ful$', w)) :\n",
    "                w = re.sub('ful$', '', w)\n",
    "            elif (re.search('ness$', w)) :\n",
    "                w = re.sub('ness$', '', w)\n",
    "        # Step 4\n",
    "        if (m_cnt > 1) :\n",
    "            if (re.search('al$', w)) :\n",
    "                w = re.sub('al$', '', w)\n",
    "            elif (re.search('ance$', w)) :\n",
    "                w = re.sub('ance$', '', w)\n",
    "            elif (re.search('ence$', w)) :\n",
    "                w = re.sub('ence$', '', w)\n",
    "            elif (re.search('er$', w)) :\n",
    "                w = re.sub('er$', '', w)\n",
    "            elif (re.search('ic$', w)) :\n",
    "                w = re.sub('ic$', '', w)\n",
    "            elif (re.search('able$', w)) :\n",
    "                w = re.sub('able$', '', w)\n",
    "            elif (re.search('ible$', w)) :\n",
    "                w = re.sub('ible$', '', w)\n",
    "            elif (re.search('ant$', w)) :\n",
    "                w = re.sub('ant$', '', w)\n",
    "            elif (re.search('ement$', w)) :\n",
    "                w = re.sub('ement$', '', w)\n",
    "            elif (re.search('ent$', w)) :\n",
    "                w = re.sub('ent$', '', w)\n",
    "            elif (m_cnt > 1 and re.search('[s|t]ion$', w)) :\n",
    "                w = re.sub('[s|t]ion$', '', w)\n",
    "            elif (re.search('ou$', w)) :\n",
    "                w = re.sub('ou$', '', w)\n",
    "            elif (re.search('ism$', w)) :\n",
    "                w = re.sub('ism$', '', w)\n",
    "            elif (re.search('ate$', w)) :\n",
    "                w = re.sub('ate$', '', w)\n",
    "            elif (re.search('iti$', w)) :\n",
    "                w = re.sub('iti$', '', w)\n",
    "            elif (re.search('ous$', w)) :\n",
    "                w = re.sub('ous$', '', w)\n",
    "            elif (re.search('ive$', w)) :\n",
    "                w = re.sub('ive$', '', w)\n",
    "            elif (re.search('ize$', w)) :\n",
    "                w = re.sub('ize$', '', w)\n",
    "        # Step 5a\n",
    "        if (m_cnt > 1 and re.search('e$', w)) :\n",
    "            w = re.sub('e$', '', w)\n",
    "        # measure at least one and ends in cvc\n",
    "        # but second c is not W,X,Y\n",
    "        elif (m_cnt == 1 and not re.search('^[^aeiou][aeiou][^aeiouwxy]e$', w)) :\n",
    "            w = re.sub('e$', '', w)\n",
    "        # Step 5b\n",
    "        if (m_cnt > 1 and re.search('.*ll$', w)):\n",
    "            w = re.sub('l$', '', w)\n",
    "        # end of porter stemming\n",
    "        # attach the word back to the new string\n",
    "        new_s += (\" \" + w)\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopword_preprocessor(s):    \n",
    "    # remove \"stop\" words which bear no significant meaning in most contexts\n",
    "    s = re.sub(' ?the ', r' ', s)\n",
    "    s = re.sub(' ?who ', r' ', s)\n",
    "    s = re.sub(' ?what ', r' ', s)\n",
    "    s = re.sub(' ?them ', r' ', s)\n",
    "    s = re.sub(' ?my ', r' ', s)\n",
    "    s = re.sub(' ?our ', r' ', s)\n",
    "    s = re.sub(' ?this ', r' ', s)\n",
    "    s = re.sub(' ?that ', r' ', s)\n",
    "    s = re.sub(' ?which ', r' ', s)\n",
    "    s = re.sub(' ?why ', r' ', s)\n",
    "    s = re.sub(' ?me ', r' ', s)\n",
    "    #s = re.sub(' ?i ', r' ', s)\n",
    "    #s = re.sub(' ?us ', r' ', s)\n",
    "    #s = re.sub(' ?you ', r' ', s)\n",
    "    s = re.sub(' ?they ', r' ', s)\n",
    "    s = re.sub(' ?where ', r' ', s)\n",
    "    s = re.sub(' ?and ', r' ', s)\n",
    "    s = re.sub(' ?for ', r' ', s)\n",
    "    s = re.sub(' ?his ', r' ', s)\n",
    "    s = re.sub(' ?her ', r' ', s)\n",
    "    s = re.sub(' ?to ', r' ', s)\n",
    "    #s = re.sub(' ?of ', r' ', s)\n",
    "    \n",
    "    # let's also get rid of 'request' and 'pizza' since they show up frequently in titles\n",
    "    s = re.sub('request', r'', s)\n",
    "    s = re.sub('pizza', r'', s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic string preprocessor:\n",
    "# lowercases all text, removes, digits and special characters\n",
    "def basic_preprocessor(s):\n",
    "    # make everything lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # eliminate consecutive digits\n",
    "    s = re.sub('([0-9])[0-9]+', r'\\1', s)\n",
    "    \n",
    "    # eliminate all digits\n",
    "    #s = re.sub('[0-9]+', r'', s)\n",
    "    \n",
    "    # eliminate special characters, except hyphens\n",
    "    s = re.sub('[^A-Za-z0-9\\s\\-]+', ' ', s)\n",
    "    \n",
    "    # add a space between consecutive numbers/alpha\n",
    "    s = re.sub('([0-9])([a-z])', '\\1 \\2', s)\n",
    "    s = re.sub('([a-z])([0-9])', '\\1 \\2', s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the other preprocessors already written above\n",
    "# to form a composite preprocessor\n",
    "def composite_preprocessor(s):\n",
    "    s = basic_preprocessor(s)\n",
    "    s = stopword_preprocessor(s)\n",
    "    #s = porter_stemming(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed different preprocessors with different functionalities:\n",
    "- Basic Preprocessor\n",
    " - Lower case text, removes consecutive digits\n",
    "- Stopword Preprocessor\n",
    " - Removes common, non contextual stopwords (the, that, and... etc)\n",
    "- Porter-Stemming Preprocessor\n",
    " - Advanced preprocessing of vocabulary, suffix removal\n",
    " - Example: learning -> learn\n",
    "\n",
    "We retrained new basic models, this time we use an updated vocabulary based on our vectorizer preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 70.1%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 81.24\n",
      "True: 26.34\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.88%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.81\n",
      "True: 22.83\n",
      "\n",
      "The accuracy of Decision Tree model is 65.94%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 77.84\n",
      "True: 26.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(tf_pp_train, tf_pp_dev) = vectorize_with_preprocessor(composite_preprocessor)\n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_pp_lr = LogisticRegression()\n",
    "basic_pp_nb = BernoulliNB()\n",
    "basic_pp_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_pp_lr, 'Logistic Regression'), (basic_pp_nb, 'Naive Bayes'),\n",
    "                  (basic_pp_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_pp_train, train_labels,\n",
    "                                                  tf_pp_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our testing, we found the best results by compositing our basic preprocessor with the stopword preprocessor.  The Porter-Stemming preprocessor seemed to actually have an adverse impact in accuracy, albeit, marginal.\n",
    "\n",
    "Using the composite preprocessor (basic and stopword), the accuracy of for our models changed:\n",
    "- Logistic Regression model from 69.21% to 70.1%\n",
    "- Naive Bayes model from 71.98% to 71.88%\n",
    "- Decision Tree model from 67.72% to 64.75%\n",
    "\n",
    "Although the addition of preprocessing had only marginal impacts to Logistic Regression and Naive Bayes, in our dataset, it appears to have improved Logistic Regression, if only slightly and had a negative affect for Naive Bayes, also only slightly.  Decision Tree modeling was more adversely affected by the introduction of preprocessing.\n",
    "\n",
    "Overall, Naive Bayes appears to be performing better than the Logistic Regression and Decision Tree models, although only about 2% better over the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 74.85%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.45\n",
      "True: 7.3\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.88%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.81\n",
      "True: 22.83\n",
      "\n",
      "The accuracy of Decision Tree model is 63.76%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 75.98\n",
      "True: 26.21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize textual data with TF-IDF transformation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorizer(preprocessor_func, ngram=(1,1)):\n",
    "    # use title and text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data['request_title'] + train_data[text_column]\n",
    "    dev_text = dev_data['request_title'] + dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tfidf_vect = TfidfVectorizer(preprocessor=preprocessor_func, ngram_range=ngram)\n",
    "    tfidf = tfidf_vect.fit(train_text)\n",
    "    \n",
    "    tfidf_train = tfidf_vect.transform(train_text)\n",
    "    tfidf_dev = tfidf_vect.transform(dev_text)\n",
    "    \n",
    "    return (tfidf_train, tfidf_dev)\n",
    "\n",
    "(tfidf_train, tfidf_dev) = tfidf_vectorizer(composite_preprocessor)\n",
    "\n",
    "basic_tfidf_lr = LogisticRegression()\n",
    "basic_tfidf_nb = BernoulliNB()\n",
    "basic_tfidf_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_tfidf_lr, 'Logistic Regression'),\n",
    "                          (basic_tfidf_nb, 'Naive Bayes'),\n",
    "                          (basic_tfidf_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model,\n",
    "                                                  tfidf_train, train_labels,\n",
    "                                                  tfidf_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted to also try a TF-IDF (Term Frequncy) vectorizer for our training vocabulary to see if using a different vectorizer method would bear significant improvment to our models.  We found a good improvment in overall accuracy of our Logistic Regression model but did not find any signifcant differences in applying a different vectorizer to Naive Bayes whereas  Decision Tree model was adversely affected:\n",
    " - Logistic Regression 70.1 to 74.85\n",
    " - Naive Bayes 71.88 to 71.88\n",
    " - Decision Tree 64.75 to 62.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional potential improvment to both models is to modify our count vectorizer to include bi-gram text, as opposed to uni-gram text which is currently being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Count Vectorizer and bi-gram vocabulary:\n",
      "The accuracy of Logistic Regression model is 70.59%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.03\n",
      "True: 19.07\n",
      "\n",
      "The accuracy of Naive Bayes model is 73.96%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.0\n",
      "True: 1.5\n",
      "\n",
      "The accuracy of Decision Tree model is 68.71%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 80.32\n",
      "True: 23.67\n",
      "\n",
      "Using TF-IDF Vectorizer and bi-gram vocabulary:\n",
      "The accuracy of Logistic Regression model is 74.46%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.34\n",
      "True: 0.77\n",
      "\n",
      "The accuracy of Naive Bayes model is 73.96%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.0\n",
      "True: 1.5\n",
      "\n",
      "The accuracy of Decision Tree model is 63.27%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 75.12\n",
      "True: 29.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Using Count Vectorizer and bi-gram vocabulary:\"\n",
    "bi_gram = (1,2)\n",
    "(tf_bi_train, tf_bi_dev) = vectorize_with_preprocessor(composite_preprocessor, bi_gram)\n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_bi_lr = LogisticRegression()\n",
    "basic_bi_nb = BernoulliNB()\n",
    "basic_bi_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_bi_lr, 'Logistic Regression'), (basic_bi_nb, 'Naive Bayes'),\n",
    "                  (basic_bi_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_bi_train, train_labels,\n",
    "                                                  tf_bi_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n",
    "    \n",
    "print \"Using TF-IDF Vectorizer and bi-gram vocabulary:\"\n",
    "(tfidf_bi_train, tfidf_bi_dev) = tfidf_vectorizer(composite_preprocessor, bi_gram)\n",
    "\n",
    "basic_tfidf_bi_lr = LogisticRegression()\n",
    "basic_tfidf_bi_nb = BernoulliNB()\n",
    "basic_tfidf_bi_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_tfidf_bi_lr, 'Logistic Regression'),\n",
    "                          (basic_tfidf_bi_nb, 'Naive Bayes'),\n",
    "                          (basic_tfidf_bi_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model,\n",
    "                                                  tfidf_bi_train, train_labels,\n",
    "                                                  tfidf_bi_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bi-gram vocabularies, using both the Count and TF-IDF vectorizers had mixed results, with marginal improvments and degredation in overall accuracy for various models.  It should also be noted that when there was improvment, the F1 score for sucessful pizza requests dropped significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will attempt to tune our default modeling by gridsearching over various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'C': 0.001}\n",
      "The accuracy of Tuned Logistic Regression with CV model is 74.75%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.35\n",
      "True: 8.6\n",
      "\n",
      "Best parameters:\n",
      "{'C': 0.001}\n",
      "The accuracy of Tuned Logistic Regression with TF-IDF model is 74.55%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.42\n",
      "True: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgin/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# generic grid search which can field different models and hyperparameters\n",
    "# and output the best results\n",
    "def gridsearch_model(model, parameters, tf_dev, dev_labels): \n",
    "    gridsearch = GridSearchCV(estimator=model,\n",
    "                              param_grid=parameters)\n",
    "    gridsearch.fit(tf_dev, dev_labels)\n",
    "    \n",
    "    print \"Best parameters:\"\n",
    "    print gridsearch.best_params_\n",
    "    \n",
    "# c_values for logistic regression\n",
    "c_values = {'C': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0,\n",
    "                  5.0, 10.0, 15.0, 20.0, 25.0, 50.0, 75.0, 100.0]}\n",
    "\n",
    "# logistic regression tuning\n",
    "gridsearch_model(basic_pp_lr, c_values, tf_pp_dev, dev_labels)\n",
    "\n",
    "# use optimal parameters\n",
    "tuned_lr = LogisticRegression(C=0.01)\n",
    "tuned_lr.fit(tf_pp_train, train_labels)\n",
    "lr_accuracy = tuned_lr.score(tf_pp_dev, dev_labels) \n",
    "lr_predicted = tuned_lr.predict(tf_pp_dev)\n",
    "lr_f1_score = metrics.f1_score(lr_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Logistic Regression with CV\", lr_accuracy, lr_f1_score)\n",
    "\n",
    "gridsearch_model(basic_tfidf_lr, c_values, tfidf_dev, dev_labels)\n",
    "\n",
    "# use optimal parameters\n",
    "tuned_tfidf_lr = LogisticRegression(C=0.01)\n",
    "tuned_tfidf_lr.fit(tfidf_train, train_labels)\n",
    "lr_tfidf_accuracy = tuned_tfidf_lr.score(tfidf_dev, dev_labels) \n",
    "lr_tfidf_predicted = tuned_tfidf_lr.predict(tfidf_dev)\n",
    "lr_tfidf_f1_score = metrics.f1_score(lr_tfidf_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Logistic Regression with TF-IDF\", lr_tfidf_accuracy, lr_tfidf_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'alpha': 0.95}\n",
      "The accuracy of Tuned Naive Bayes model is 71.58%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.49\n",
      "True: 24.67\n",
      "\n",
      "Best parameters:\n",
      "{'alpha': 0.95}\n",
      "The accuracy of Tuned Naive Bayes with TF-IDF model is 71.58%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.42\n",
      "True: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# naive bayes tuning\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0,]}\n",
    "gridsearch_model(basic_pp_nb, params, tf_pp_dev, dev_labels)\n",
    "\n",
    "tuned_nb = BernoulliNB(alpha=0.85)\n",
    "tuned_nb.fit(tf_pp_train, train_labels)\n",
    "nb_accuracy = tuned_nb.score(tf_pp_dev, dev_labels)\n",
    "nb_predicted = tuned_nb.predict(tf_pp_dev)\n",
    "nb_f1_score = metrics.f1_score(nb_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Naive Bayes\", nb_accuracy, nb_f1_score)\n",
    "\n",
    "# TF-IDF vocabulary w/ Naive Bayes tuning\n",
    "gridsearch_model(basic_tfidf_nb, params, tf_pp_dev, dev_labels)\n",
    "# use optimal parameters\n",
    "tuned_tfidf_nb = BernoulliNB(alpha=0.85)\n",
    "tuned_tfidf_nb.fit(tfidf_train, train_labels)\n",
    "nb_tfidf_accuracy = tuned_tfidf_nb.score(tfidf_dev, dev_labels) \n",
    "nb_tfidf_predicted = tuned_tfidf_lr.predict(tfidf_dev)\n",
    "nb_tfidf_f1_score = metrics.f1_score(nb_tfidf_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Naive Bayes with TF-IDF\", nb_tfidf_accuracy, nb_tfidf_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, using grid search did not find us hyperparameter values, C and alpha, for Logistic Regression and Naive Bayes, respectively, that improved overall accuracy of the models. Trial and error proved to be more effective.\n",
    "\n",
    "In the case of tuning Naive Bayes, the differences in accuracy was marginal.  It's also noted that using a TF-IDF model vs Count Vectorized vocabulary features did not produce significant improvements while tuning the model.\n",
    "\n",
    "Logistic Regression showed slight improvement with tuning, however, we are getting 0% accuracy for \"True\" or successful pizza requests, meaning while the model has good accuracy overall with the tuned parameters, its accuracy for predicting sucessful requests is zero.  Therefore the tuning of parameters to values found in gridsearch is not ideal and requires some manual finessing.\n",
    "\n",
    "Another reason, it is possible there is a class imbalance in our training data and the model is merely always guessing False/unsuccessful pizza requests to obtain a higher accuracy.  If there is a class imbalance in our training data, it may be difficult for the model to distinguish between sucessful and unsucessful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2293\n",
      "1 737\n"
     ]
    }
   ],
   "source": [
    "# doing some exploration to find out if there is a class imbalance in our training data\n",
    "# by counting the outcome classes in our training set\n",
    "def class_counts():\n",
    "    counts = [0, 0]\n",
    "    for i in train_labels:\n",
    "        counts[i] += 1\n",
    "    \n",
    "    for i in range(len(counts)):\n",
    "        print i, counts[i]\n",
    "        \n",
    "class_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from our class counting that there is indeed some weight imbalance in our training set.   There are far more classes in the unsucessful requests compared to the sucessful requests, roughly 3 to 1, in the favor of unsuccessful requests.  We can use this knowledge to weight our classes for training to improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Weighed Logistic Regression model is 74.55%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.34\n",
      "True: 3.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There is some class imbalance, so lets try weighting based on the ratio of classes\n",
    "weight_dict = {0: 0.33, 1: .67}\n",
    "lr_weight = LogisticRegression(C=0.001, class_weight=weight_dict)\n",
    "lr_weight.fit(tf_pp_train, train_labels)\n",
    "lrw_accuracy = lr_weight.score(tf_pp_dev, dev_labels) \n",
    "lrw_predicted = lr_weight.predict(tf_pp_dev)\n",
    "lrw_f1_score = metrics.f1_score(lrw_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Weighed Logistic Regression\", lrw_accuracy, lrw_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adjusting the weighting for logistic improved show only marginal improvement for the True or sucessful request prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 76.63%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 84.99\n",
      "True: 47.32\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.98%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.88\n",
      "True: 22.89\n",
      "\n",
      "The accuracy of Decision Tree model is 80.1%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 86.8\n",
      "True: 59.56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# introduce other features from the dataset to combine with text features we extracted\n",
    "# from the message bodies\n",
    "import scipy as sp\n",
    "\n",
    "num_train_examples = train_data.shape[0]\n",
    "num_dev_examples = dev_data.shape[0]\n",
    "\n",
    "# We start with the preprocessed vocabulary as a basis\n",
    "\n",
    "tf_train_plus = sp.sparse.hstack((\n",
    "                    tf_pp_train, \n",
    "                    train_data['number_of_upvotes_of_request_at_retrieval'].values.reshape(num_train_examples, 1), \n",
    "                    train_data['request_number_of_comments_at_retrieval'].values.reshape(num_train_examples, 1),\n",
    "                    train_data['requester_number_of_posts_on_raop_at_retrieval'].values.reshape(num_train_examples, 1),\n",
    "                    #train_data['requester_days_since_first_post_on_raop_at_request'].values.reshape(num_train_examples, 1),\n",
    "                ), format='csr')\n",
    "\n",
    "tf_dev_plus = sp.sparse.hstack((\n",
    "                    tf_pp_dev, \n",
    "                    dev_data['number_of_upvotes_of_request_at_retrieval'].values.reshape(num_dev_examples, 1), \n",
    "                    dev_data['request_number_of_comments_at_retrieval'].values.reshape(num_dev_examples, 1),\n",
    "                    dev_data['requester_number_of_posts_on_raop_at_retrieval'].values.reshape(num_dev_examples, 1),\n",
    "                    #dev_data['requester_days_since_first_post_on_raop_at_request'].values.reshape(num_dev_examples, 1),\n",
    "                ), format='csr')\n",
    "    \n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "# calling af for added features\n",
    "basic_af_lr = LogisticRegression()\n",
    "basic_af_nb = BernoulliNB()\n",
    "basic_af_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_af_lr, 'Logistic Regression'), (basic_af_nb, 'Naive Bayes'),\n",
    "                  (basic_af_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_train_plus, train_labels,\n",
    "                                                  tf_dev_plus, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Attribue Name                Model    Acc False  True\n",
      "       number_of_downvotes_of_request_at_retrieval  Logistic Regression 0.7010 0.8129 0.2562\n",
      "                                                            Naive Bayes 0.7178 0.8276 0.2234\n",
      "                                                          Decision Tree 0.6673 0.7838 0.2790\n",
      "         number_of_upvotes_of_request_at_retrieval  Logistic Regression 0.7040 0.8146 0.2654\n",
      "                                                            Naive Bayes 0.7188 0.8281 0.2283\n",
      "                                                          Decision Tree 0.6703 0.7845 0.2989\n",
      "                                   post_was_edited  Logistic Regression 0.7455 0.8542 0.0000\n",
      "                                                            Naive Bayes 0.7188 0.8281 0.2283\n",
      "                                                          Decision Tree 0.6624 0.7799 0.2760\n",
      "           request_number_of_comments_at_retrieval  Logistic Regression 0.7099 0.8161 0.3138\n",
      "                                                            Naive Bayes 0.7198 0.8288 0.2289\n",
      "                                                          Decision Tree 0.7109 0.8104 0.3917\n",
      "          requester_account_age_in_days_at_request  Logistic Regression 0.7010 0.8122 0.2670\n",
      "                                                            Naive Bayes 0.7188 0.8283 0.2240\n",
      "                                                          Decision Tree 0.6634 0.7792 0.2917\n",
      "        requester_account_age_in_days_at_retrieval  Logistic Regression 0.6970 0.8095 0.2609\n",
      "                                                            Naive Bayes 0.7188 0.8281 0.2283\n",
      "                                                          Decision Tree 0.6782 0.7926 0.2826\n",
      "requester_days_since_first_post_on_raop_at_request  Logistic Regression 0.7000 0.8112 0.2699\n",
      "                                                            Naive Bayes 0.7178 0.8276 0.2234\n",
      "                                                          Decision Tree 0.6653 0.7817 0.2839\n",
      "requester_days_since_first_post_on_raop_at_retriev  Logistic Regression 0.6871 0.8015 0.2617\n",
      "                                                            Naive Bayes 0.7188 0.8281 0.2283\n",
      "                                                          Decision Tree 0.6644 0.7837 0.2517\n",
      "           requester_number_of_comments_at_request  Logistic Regression 0.6950 0.8087 0.2488\n",
      "                                                            Naive Bayes 0.7178 0.8276 0.2234\n",
      "                                                          Decision Tree 0.6614 0.7802 0.2629\n",
      "         requester_number_of_comments_at_retrieval  Logistic Regression 0.7010 0.8120 0.2705\n",
      "                                                            Naive Bayes 0.7198 0.8290 0.2247\n",
      "                                                          Decision Tree 0.6772 0.7913 0.2882\n",
      "   requester_number_of_comments_in_raop_at_request  Logistic Regression 0.7040 0.8149 0.2617\n",
      "                                                            Naive Bayes 0.7178 0.8276 0.2234\n",
      "                                                          Decision Tree 0.6802 0.7936 0.2901\n",
      " requester_number_of_comments_in_raop_at_retrieval  Logistic Regression 0.7208 0.8238 0.3286\n",
      "                                                            Naive Bayes 0.7218 0.8302 0.2301\n",
      "                                                          Decision Tree 0.6941 0.7958 0.3905\n",
      "              requester_number_of_posts_at_request  Logistic Regression 0.7050 0.8154 0.2660\n",
      "                                                            Naive Bayes 0.7178 0.8276 0.2234\n",
      "                                                          Decision Tree 0.6693 0.7867 0.2643\n",
      "            requester_number_of_posts_at_retrieval  Logistic Regression 0.7050 0.8154 0.2660\n",
      "                                                            Naive Bayes 0.7188 0.8281 0.2283\n",
      "                                                          Decision Tree 0.6851 0.7951 0.3205\n",
      "      requester_number_of_posts_on_raop_at_request  Logistic Regression 0.7030 0.8130 0.2788\n",
      "                                                            Naive Bayes 0.7178 0.8276 0.2234\n",
      "                                                          Decision Tree 0.6802 0.7909 0.3200\n",
      "    requester_number_of_posts_on_raop_at_retrieval  Logistic Regression 0.7693 0.8534 0.4594\n",
      "                                                            Naive Bayes 0.7188 0.8281 0.2283\n",
      "                                                          Decision Tree 0.7703 0.8511 0.4978\n",
      "         requester_number_of_subreddits_at_request  Logistic Regression 0.6990 0.8116 0.2512\n",
      "                                                            Naive Bayes 0.7188 0.8283 0.2240\n",
      "                                                          Decision Tree 0.6663 0.7819 0.2905\n",
      "      requester_upvotes_minus_downvotes_at_request  Logistic Regression 0.6941 0.8082 0.2445\n",
      "                                                            Naive Bayes 0.7188 0.8283 0.2240\n",
      "                                                          Decision Tree 0.6663 0.7858 0.2461\n",
      "    requester_upvotes_minus_downvotes_at_retrieval  Logistic Regression 0.6950 0.8080 0.2596\n",
      "                                                            Naive Bayes 0.7198 0.8288 0.2289\n",
      "                                                          Decision Tree 0.6683 0.7829 0.2977\n",
      "       requester_upvotes_plus_downvotes_at_request  Logistic Regression 0.6990 0.8112 0.2585\n",
      "                                                            Naive Bayes 0.7188 0.8283 0.2240\n",
      "                                                          Decision Tree 0.6614 0.7802 0.2629\n",
      "     requester_upvotes_plus_downvotes_at_retrieval  Logistic Regression 0.7059 0.8159 0.2703\n",
      "                                                            Naive Bayes 0.7188 0.8281 0.2283\n",
      "                                                          Decision Tree 0.6812 0.7944 0.2907\n",
      "                         unix_timestamp_of_request  Logistic Regression 0.7455 0.8542 0.0000\n",
      "                                                            Naive Bayes 0.7188 0.8281 0.2283\n",
      "                                                          Decision Tree 0.6673 0.7818 0.3000\n",
      "                     unix_timestamp_of_request_utc  Logistic Regression 0.7455 0.8542 0.0000\n",
      "                                                            Naive Bayes 0.7188 0.8281 0.2283\n",
      "                                                          Decision Tree 0.6762 0.7897 0.2968\n"
     ]
    }
   ],
   "source": [
    "# We've progressively showed mining features from text, maybe we should also show more about\n",
    "# how other features were experimented with...\n",
    "\n",
    "num_train_examples = train_data.shape[0]\n",
    "num_dev_examples = dev_data.shape[0]\n",
    "\n",
    "# We start with the preprocessed vocabulary as a basis\n",
    "# but we add a single field from the dataset\n",
    "def addFeature(field, tf_train, tf_dev):\n",
    "    new_train_col = train_data[field].values.reshape(num_train_examples,1)\n",
    "    new_dev_col = dev_data[field].values.reshape(num_dev_examples,1)\n",
    "    tf_train_plus = sp.sparse.hstack((tf_train, \n",
    "                                     new_train_col, \n",
    "                                     ), format='csr')\n",
    "    tf_dev_plus = sp.sparse.hstack((tf_dev, \n",
    "                                   new_dev_col, \n",
    "                                   ), format='csr')\n",
    "    return tf_train_plus, tf_dev_plus\n",
    "\n",
    "# print the table header\n",
    "print '%50.50s %20.20s %6.6s %5.5s %5.5s' %(\"Attribue Name\", \"Model\", \"Acc\", \"False\", \"True\")\n",
    "\n",
    "# Loop through all the different attribute fields in the dataset,\n",
    "# picking one out at a time to add to the feature vector to see how a single one improves (or not)\n",
    "for k in sorted(train_data.columns):\n",
    "    # skip these since we already have the text features\n",
    "    if (k == \"request_text\" or k == \"request_title\"):\n",
    "        continue\n",
    "    # these are giving the transform some compile errors, skipping them\n",
    "    elif (k == \"giver_username_if_known\" or\n",
    "          k == \"request_id\" or\n",
    "          k == \"request_text_edit_aware\" or\n",
    "          k == \"requester_subreddits_at_request\" or\n",
    "          k == \"requester_user_flair\" or\n",
    "          k == \"requester_username\"):\n",
    "        continue\n",
    "    # train basic models to gauge baseline performance of the models\n",
    "    # Logisitc Regression, Naive Bayes, Decision Tree\n",
    "    # calling af for added features\n",
    "    basic_af_lr = LogisticRegression()\n",
    "    basic_af_nb = BernoulliNB()\n",
    "    basic_af_dt = DecisionTreeClassifier()\n",
    "\n",
    "    # use this to flag if its the first model to print in the attribute\n",
    "    # used for formatting the print later\n",
    "    att_flag = 0\n",
    "    \n",
    "    tf_train_plus, tf_dev_plus = addFeature(k, tf_pp_train, tf_pp_dev)\n",
    "    \n",
    "    for model, model_name in [(basic_af_lr, 'Logistic Regression'), (basic_af_nb, 'Naive Bayes'),\n",
    "                      (basic_af_dt, 'Decision Tree')]:\n",
    "        accuracy, f1_score = train_and_evaluate_model(model, tf_train_plus, train_labels,\n",
    "                                                      tf_dev_plus, dev_labels)\n",
    "        #print_model_scores(model_name, accuracy, f1_score)\n",
    "        \n",
    "        # print a table of values\n",
    "        # attribute       model     accuracy f1_false f1_true\n",
    "        #                 model     accuracy f1_false f1_true\n",
    "        if (att_flag == 1):\n",
    "            print '%50.50s %20.20s %1.4f %1.4f %1.4f' %(\"\", model_name, accuracy, f1_score[0], f1_score[1])\n",
    "        else:\n",
    "            print '%50.50s %20.20s %1.4f %1.4f %1.4f' %(k, model_name, accuracy, f1_score[0], f1_score[1]) \n",
    "            att_flag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By introducing additional features from our dataset beyond the textual vocabulary, we were able to improve accuracy from our \"basic\" models.\n",
    "- Logistic Regression model from 69.21% to 70.99%\n",
    "- Naive Bayes model from 71.98% to 71.98%\n",
    "- Decision Tree model from 67.72% to 71.58%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 74.55%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.15\n",
      "True: 11.07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try combining some things for Logistic Regression\n",
    "# Using the uni-gram vocabulary plus additional features\n",
    "# We'll tune the model's C value (hyper parameter)\n",
    "# And apply weighting to see what kind of accuracy we can produce\n",
    "\n",
    "weight_dict = {0: 0.4, 1: .6}\n",
    "lr_final = LogisticRegression(C=0.01, class_weight=weight_dict)\n",
    "\n",
    "#gridsearch_model(lr_final, c_values, tf_dev_plus, dev_labels)\n",
    "\n",
    "lr_final.fit(tf_train_plus, train_labels)\n",
    "lr_final_accuracy = lr_final.score(tf_dev_plus, dev_labels) \n",
    "lr_final_predicted = lr_final.predict(tf_dev_plus)\n",
    "lr_final_f1 = metrics.f1_score(lr_final_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Logistic Regression\", lr_final_accuracy, lr_final_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By combing various improvments to the Logistic Regression model, we were able to predict with approximately 75% accuracy of our development dataset.  This included tuning of hyperparameters, weighting the training dataset for class imbalance, and introducing features beyond the textual vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next attempt to improve the performance of our Logistic Regression model by first training such a model with L1 regularization and using only the features in that model that are non-zero as the input for a model that uses L2 regularization. We are effectively removing unimportant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11682 features have been removed, and 90 remain.\n",
      "The accuracy of Logistic Regression model is 74.65%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.15\n",
      "True: 11.07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit an L1 model \n",
    "lr_with_l1 = LogisticRegression(penalty='l1', C=.1, tol=.01)\n",
    "lr_with_l1.fit(tf_train_plus, train_labels)\n",
    "\n",
    "# find the features whose weights have not been reduced to 0\n",
    "non_zero_feature_indices = lr_with_l1.coef_.nonzero()[1]\n",
    "\n",
    "# use only those features as the input to the L2 model\n",
    "tf_train_reduced = tf_train_plus[:, non_zero_feature_indices]\n",
    "tf_dev_reduced = tf_dev_plus[:, non_zero_feature_indices]\n",
    "\n",
    "# see how many features we've eliminated\n",
    "features_remaining = tf_train_reduced.shape[1]\n",
    "\n",
    "print '{} features have been removed, and {} remain.'.format(\n",
    "    tf_train_plus.shape[1] - features_remaining, features_remaining) \n",
    "\n",
    "# score the revised model\n",
    "lr_reduced = LogisticRegression(C=0.01, class_weight=weight_dict).fit(tf_train_reduced, train_labels)\n",
    "lr_predicted = lr_reduced.predict(tf_dev_reduced)\n",
    "lr_accuracy = lr_reduced.score(tf_dev_reduced, dev_labels)\n",
    "lr_f1_score = metrics.f1_score(lr_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Logistic Regression\", lr_accuracy, lr_final_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApcAAAHHCAYAAADqCnP7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118dd65d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: this is place holder code for now\n",
    "\n",
    "# bar graph of the different accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Example data\n",
    "models = ('Logistic Regression', 'Naive Bayes', 'Decision Tree')\n",
    "y_pos = np.arange(len(models))\n",
    "# TODO: fill in with actual accuracy from various model permutations\n",
    "accuracy_scores = [70,80,60]\n",
    "error = [5,5,5]\n",
    "\n",
    "#ax.barh(y_pos, accuracy_scores, xerr=error, align='center',\n",
    "#        color='green', ecolor='black')\n",
    "ax.barh(y_pos, accuracy_scores, align='center',\n",
    "        color='green')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(models)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Model Accuracies')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Group Project/Final\n",
    "## Kaggle Competition: Random Acts of Pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Members:<br>\n",
    "Daniel Elkin<br>\n",
    "Mark Gin\n",
    "\n",
    "Project Prompt:<br>\n",
    "People post pizza requests on Reddit<br>\n",
    "Build 2-class classifier<br>\n",
    "Classify whether post will get pizza<br>\n",
    "Practice mining features from text<br>\n",
    "\n",
    "Reference links:\n",
    " - https://www.kaggle.com/c/random-acts-of-pizza\n",
    " - http://cs.stanford.edu/~althoff/raop-dataset/\n",
    "\n",
    "Data Set:<br>\n",
    "This training dataset contains a collection of 5671 textual requests for pizza from the Reddit community \"Random Acts of Pizza\" together with their outcome (successful/unsuccessful) and meta-data.\n",
    "\n",
    "We will split the dataset into:\n",
    " - 25% for development\n",
    " - 75% for training\n",
    "\n",
    "A separate dataset file was provided for testing purposes, of which we do not have the labels as to whether or not a pizza was received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "df_train = pd.read_json('train.json')\n",
    "df_test = pd.read_json('test.json')\n",
    "\n",
    "# drop the target column from the data and use it for the labels\n",
    "classification_column_name = 'requester_received_pizza'\n",
    "\n",
    "train_data = df_train.drop([classification_column_name], axis=1)\n",
    "train_labels = df_train[classification_column_name]\n",
    "\n",
    "# use twenty-five percent of the training data for a dev data set\n",
    "# note that we cannot use the test data set here, because we are not given their labels\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(train_data, train_labels, random_state=42)\n",
    "\n",
    "# global dictionary of accuracies so we can plot our progress\n",
    "accuracy_dict = {}\n",
    "# keeping a list of model modification names, so we can plot them in a particular order\n",
    "modification = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be parsing the message body of each pizza request to utilize as features for our models, we will create term-frequency matricies of the text to use in our models as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# debug function used to print the vocabularies into a text file\n",
    "def output_file(output_name, output_list):\n",
    "    with open(output_name, 'w') as output:\n",
    "        for i in output_list:\n",
    "            output.write(i.encode('UTF-8') + \"\\n\")\n",
    "\n",
    "def decimal_to_percent(decimal):\n",
    "    return round(decimal * 100, 2)\n",
    "\n",
    "def basic_vectorizer():\n",
    "    ''' Construct term-frequency matrices for use in models '''\n",
    "    \n",
    "    # use title and text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data['request_title'] + train_data[text_column]\n",
    "    dev_text = dev_data['request_title'] + dev_data[text_column]\n",
    "#    train_text = train_data[text_column]\n",
    "#    dev_text = dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tf_vect = CountVectorizer()\n",
    "    tf_train = tf_vect.fit_transform(train_text)\n",
    "    tf_dev = tf_vect.transform(dev_text)\n",
    "    \n",
    "    #output_file(\"basic_vocab.txt\", tf_vect.get_feature_names())\n",
    "    \n",
    "    return (tf_train, tf_dev)\n",
    "\n",
    "(tf_train, tf_dev) = basic_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train basic models, without tuning, for each candidate learning model:\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Decision Tree\n",
    "  \n",
    "We will train and find the accuracies of each model.  This will give us a general idea of which learning models may be most successful and can build upon them from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, tf_train, train_labels, tf_dev, dev_labels):\n",
    "    ''' Train and score a model'''\n",
    "    clf = model\n",
    "    clf.fit(tf_train, train_labels)\n",
    "    \n",
    "    # return the accuracy and F1 scores\n",
    "    accuracy = clf.score(tf_dev, dev_labels) \n",
    "    predicted = clf.predict(tf_dev)\n",
    "    f1_score = metrics.f1_score(predicted, dev_labels, average=None)\n",
    "\n",
    "    return accuracy, f1_score\n",
    "\n",
    "def print_model_scores(model_type, accuracy, f1_score):\n",
    "    ''' Print the accuracy and f1 scores '''\n",
    "    \n",
    "    print 'The accuracy of {} model is {}%'.format(model_type, decimal_to_percent(accuracy))\n",
    "    print 'The F1 scores are: False: {}\\n                    True: {}'.format(*[decimal_to_percent(score) for score in f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 69.21%\n",
      "The F1 scores are: False: 80.48\n",
      "                    True: 27.17\n",
      "The accuracy of Naive Bayes model is 71.98%\n",
      "The F1 scores are: False: 82.8\n",
      "                    True: 24.53\n",
      "The accuracy of Decision Tree model is 66.53%\n",
      "The F1 scores are: False: 78.31\n",
      "                    True: 26.84\n"
     ]
    }
   ],
   "source": [
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_lr = LogisticRegression()\n",
    "basic_nb = BernoulliNB()\n",
    "basic_dt = DecisionTreeClassifier()\n",
    "\n",
    "modification.update({0: 'Basic'})\n",
    "accuracy_dict['Basic'] = {}\n",
    "for model, model_name in [(basic_lr, 'Logistic Regression'), (basic_nb, 'Naive Bayes'),\n",
    "                  (basic_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_train, train_labels, tf_dev, dev_labels)\n",
    "    # first time adding accuracies to dictionary\n",
    "    accuracy_dict['Basic'].update({model_name: accuracy})\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Preprocessing\n",
    "Let's attempt to build upon our basic models by introducing preprocessing algorithms for our word vocabulary vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_with_preprocessor(preprocessor_func, ngram=(1,1), mindf=1):\n",
    "    ''' Construct term-frequency matrices for use in models '''\n",
    "    \n",
    "    # use title and text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data['request_title'] + train_data[text_column]\n",
    "    dev_text = dev_data['request_title'] + dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tf_vect = CountVectorizer(preprocessor=preprocessor_func,\n",
    "                              ngram_range=ngram,\n",
    "                              min_df=mindf)\n",
    "    tf = tf_vect.fit(train_text)\n",
    "    \n",
    "    # make the matrices global variables for convenience?\n",
    "    tf_train_pp = tf.transform(train_text)\n",
    "    tf_dev_pp = tf.transform(dev_text)\n",
    "    \n",
    "    #output_file(\"porter.txt\", tf_vect.get_feature_names())\n",
    "    #output_file(\"composite.txt\", tf_vect.get_feature_names())\n",
    "    \n",
    "    return (tf_train_pp, tf_dev_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use porter-stemming algorithm to generalize words in the messages\n",
    "# https://tartarus.org/martin/PorterStemmer/def.txt\n",
    "def porter_stemming(s):\n",
    "    # create a new empty string, since s is the entire message not a single word\n",
    "    new_s = \"\"\n",
    "    \n",
    "    # iterate through each word in space delimited string\n",
    "    for w in s.split():\n",
    "        # calculate the measure, which is the number of vowel to consanant transitions\n",
    "        m_cnt = 0\n",
    "        if (re.search('^[^aeiou]*(([aeiou]+[^aeiou]+)+)[aeiou]*$', w)):\n",
    "            measure_match = re.match('^[^aeiou]*(([aeiou]+[^aeiou]+)+)[aeiou]*$', w)\n",
    "            # split on vowels to count number of transitions\n",
    "            consanant_groups = re.split('[aeiou]+', measure_match.group(1))\n",
    "            m_cnt = len(consanant_groups) - 1\n",
    "        \n",
    "        # Step 1a of porter stemming\n",
    "        if re.search('sses$', w):\n",
    "            w = re.sub('sses$', 'ss', w)\n",
    "        elif re.search('ies$', w):\n",
    "            w = re.sub('ies$', 'i', w)\n",
    "        elif re.search('ss$', w):\n",
    "            w = re.sub('ss$', 'i', w)\n",
    "        elif re.search('s$', w):\n",
    "            w = re.sub('s$', '', w)\n",
    "        # Step 1b\n",
    "        # Porter-Stemming says this should be m_cnt > 0, but doesn't\n",
    "        # even match their own examples, tweaked to 1 and got slightly better performance\n",
    "#        if (m_cnt > 1 and re.search('eed$', w)):\n",
    "#            w = re.sub('eed$', 'ee', w)\n",
    "#        elif (re.search('.*[aeiou].*(ed|ing)$', w)):\n",
    "#            w = re.sub('(ed|ing)$', '', w)\n",
    "#            # if the second or third rule of 1b is successful, we also\n",
    "#            if (re.search('(at|bl|iz)$', w)):\n",
    "#                w += 'e'\n",
    "#            # ends in double consanant, but no l s or z\n",
    "#            elif (re.search('.*([^aeiou])([^aeiou])$', w)) :\n",
    "#                m = re.match('(.*)([^aeiou])([^aeiou])$', w)\n",
    "#                if (m.group(3) == m.group(2) and\n",
    "#                    m.group(3) != 'l' and\n",
    "#                    m.group(3) != 's' and\n",
    "#                    m.group(3) != 'z') :\n",
    "#                    w = m.group(1) + m.group(2)\n",
    "#            # measure at least one and ends in cvc\n",
    "#            # but second c is not W,X,Y\n",
    "#            elif (m_cnt == 1 and re.search('^.*[^aeiou][aeiou][^aeiouwxy]$', w)) :\n",
    "#                w = re.sub('[^aeiouwxy]$', 'e', w)\n",
    "        # Step 1c\n",
    "        if (re.search('.*[aeiou].*y$', w)) :\n",
    "            w = re.sub('y$', 'i', w)\n",
    "        # Step 2\n",
    "        if (m_cnt > 0) :\n",
    "            if (re.search('ational$', w)) :\n",
    "                w = re.sub('ational$', 'ate', w)\n",
    "            elif (re.search('tional$', w)) :\n",
    "                w = re.sub('tional$', 'tion', w)\n",
    "            elif (re.search('enci$', w)) :\n",
    "                w = re.sub('enci$', 'ence', w)\n",
    "            elif (re.search('anci$', w)) :\n",
    "                w = re.sub('anci$', 'ance', w)\n",
    "            elif (re.search('izer$', w)) :\n",
    "                w = re.sub('izer$', 'ize', w)\n",
    "            elif (re.search('abli$', w)) :\n",
    "                w = re.sub('abli$', 'able', w)\n",
    "            elif (re.search('alli$', w)) :\n",
    "                w = re.sub('alli$', 'al', w)\n",
    "            elif (re.search('entli$', w)) :\n",
    "                w = re.sub('entli$', 'ent', w)\n",
    "            elif (re.search('eli$', w)) :\n",
    "                w = re.sub('eli$', 'e', w)\n",
    "            elif (re.search('ousli$', w)) :\n",
    "                w = re.sub('ousli$', 'ous', w)\n",
    "            elif (re.search('ization$', w)) :\n",
    "                w = re.sub('ization$', 'ize', w)\n",
    "            elif (re.search('(ation|ator)$', w)) :\n",
    "                w = re.sub('(ation|ator)$', 'ate', w)\n",
    "            elif (re.search('alism$', w)) :\n",
    "                w = re.sub('alism$', 'al', w)\n",
    "            elif (re.search('iveness$', w)) :\n",
    "                w = re.sub('iveness$', 'ive', w)\n",
    "            elif (re.search('fulness$', w)) :\n",
    "                w = re.sub('fulness$', 'ful', w)\n",
    "            elif (re.search('ousness$', w)) :\n",
    "                w = re.sub('ousness$', 'ous', w)\n",
    "            elif (re.search('aliti$', w)) :\n",
    "                w = re.sub('aliti$', 'al', w)\n",
    "            elif (re.search('iviti$', w)) :\n",
    "                w = re.sub('iviti$', 'ive', w)\n",
    "            elif (re.search('biliti$', w)) :\n",
    "                w = re.sub('biliti$', 'ble', w)\n",
    "        # Step 3\n",
    "        if (m_cnt > 0) :\n",
    "            if (re.search('icate$', w)) :\n",
    "                w = re.sub('icate$', 'ic', w)\n",
    "            elif (re.search('ative$', w)) :\n",
    "                w = re.sub('ative$', '', w)\n",
    "            elif (re.search('alize$', w)) :\n",
    "                w = re.sub('alize$', 'al', w)\n",
    "            elif (re.search('iciti$', w)) :\n",
    "                w = re.sub('iciti$', 'ic', w)\n",
    "            elif (re.search('ical$', w)) :\n",
    "                w = re.sub('ical$', 'ic', w)\n",
    "            elif (re.search('ful$', w)) :\n",
    "                w = re.sub('ful$', '', w)\n",
    "            elif (re.search('ness$', w)) :\n",
    "                w = re.sub('ness$', '', w)\n",
    "        # Step 4\n",
    "        if (m_cnt > 1) :\n",
    "            if (re.search('al$', w)) :\n",
    "                w = re.sub('al$', '', w)\n",
    "            elif (re.search('ance$', w)) :\n",
    "                w = re.sub('ance$', '', w)\n",
    "            elif (re.search('ence$', w)) :\n",
    "                w = re.sub('ence$', '', w)\n",
    "            elif (re.search('er$', w)) :\n",
    "                w = re.sub('er$', '', w)\n",
    "            elif (re.search('ic$', w)) :\n",
    "                w = re.sub('ic$', '', w)\n",
    "            elif (re.search('able$', w)) :\n",
    "                w = re.sub('able$', '', w)\n",
    "            elif (re.search('ible$', w)) :\n",
    "                w = re.sub('ible$', '', w)\n",
    "            elif (re.search('ant$', w)) :\n",
    "                w = re.sub('ant$', '', w)\n",
    "            elif (re.search('ement$', w)) :\n",
    "                w = re.sub('ement$', '', w)\n",
    "            elif (re.search('ent$', w)) :\n",
    "                w = re.sub('ent$', '', w)\n",
    "            elif (m_cnt > 1 and re.search('[s|t]ion$', w)) :\n",
    "                w = re.sub('[s|t]ion$', '', w)\n",
    "            elif (re.search('ou$', w)) :\n",
    "                w = re.sub('ou$', '', w)\n",
    "            elif (re.search('ism$', w)) :\n",
    "                w = re.sub('ism$', '', w)\n",
    "            elif (re.search('ate$', w)) :\n",
    "                w = re.sub('ate$', '', w)\n",
    "            elif (re.search('iti$', w)) :\n",
    "                w = re.sub('iti$', '', w)\n",
    "            elif (re.search('ous$', w)) :\n",
    "                w = re.sub('ous$', '', w)\n",
    "            elif (re.search('ive$', w)) :\n",
    "                w = re.sub('ive$', '', w)\n",
    "            elif (re.search('ize$', w)) :\n",
    "                w = re.sub('ize$', '', w)\n",
    "        # Step 5a\n",
    "        if (m_cnt > 1 and re.search('e$', w)) :\n",
    "            w = re.sub('e$', '', w)\n",
    "        # measure at least one and ends in cvc\n",
    "        # but second c is not W,X,Y\n",
    "        elif (m_cnt == 1 and not re.search('^[^aeiou][aeiou][^aeiouwxy]e$', w)) :\n",
    "            w = re.sub('e$', '', w)\n",
    "        # Step 5b\n",
    "        if (m_cnt > 1 and re.search('.*ll$', w)):\n",
    "            w = re.sub('l$', '', w)\n",
    "        # end of porter stemming\n",
    "        # attach the word back to the new string\n",
    "        new_s += (\" \" + w)\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopword_preprocessor(s):    \n",
    "    # remove \"stop\" words which bear no significant meaning in most contexts\n",
    "    s = re.sub(' ?the ', r' ', s)\n",
    "    s = re.sub(' ?who ', r' ', s)\n",
    "    s = re.sub(' ?what ', r' ', s)\n",
    "    s = re.sub(' ?them ', r' ', s)\n",
    "    s = re.sub(' ?my ', r' ', s)\n",
    "    s = re.sub(' ?our ', r' ', s)\n",
    "    s = re.sub(' ?this ', r' ', s)\n",
    "    s = re.sub(' ?that ', r' ', s)\n",
    "    s = re.sub(' ?which ', r' ', s)\n",
    "    s = re.sub(' ?why ', r' ', s)\n",
    "    s = re.sub(' ?me ', r' ', s)\n",
    "    #s = re.sub(' ?i ', r' ', s)\n",
    "    #s = re.sub(' ?us ', r' ', s)\n",
    "    #s = re.sub(' ?you ', r' ', s)\n",
    "    s = re.sub(' ?they ', r' ', s)\n",
    "    s = re.sub(' ?where ', r' ', s)\n",
    "    s = re.sub(' ?and ', r' ', s)\n",
    "    s = re.sub(' ?for ', r' ', s)\n",
    "    s = re.sub(' ?his ', r' ', s)\n",
    "    s = re.sub(' ?her ', r' ', s)\n",
    "    s = re.sub(' ?to ', r' ', s)\n",
    "    #s = re.sub(' ?of ', r' ', s)\n",
    "    \n",
    "    # let's also get rid of 'request' and 'pizza' since they show up frequently in titles\n",
    "    s = re.sub('request', r'', s)\n",
    "    s = re.sub('pizza', r'', s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic string preprocessor:\n",
    "# lowercases all text, removes, digits and special characters\n",
    "def basic_preprocessor(s):\n",
    "    # make everything lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # eliminate consecutive digits\n",
    "    s = re.sub('([0-9])[0-9]+', r'\\1', s)\n",
    "    \n",
    "    # eliminate all digits\n",
    "    #s = re.sub('[0-9]+', r'', s)\n",
    "    \n",
    "    # eliminate special characters, except hyphens\n",
    "    s = re.sub('[^A-Za-z0-9\\s\\-]+', ' ', s)\n",
    "    \n",
    "    # add a space between consecutive numbers/alpha\n",
    "    s = re.sub('([0-9])([a-z])', '\\1 \\2', s)\n",
    "    s = re.sub('([a-z])([0-9])', '\\1 \\2', s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the other preprocessors already written above\n",
    "# to form a composite preprocessor\n",
    "def composite_preprocessor(s):\n",
    "    s = basic_preprocessor(s)\n",
    "    s = stopword_preprocessor(s)\n",
    "    #s = porter_stemming(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed different preprocessors with different functionalities:\n",
    "- Basic Preprocessor\n",
    " - Lower case text, removes consecutive digits\n",
    "- Stopword Preprocessor\n",
    " - Removes common, non contextual stopwords (the, that, and... etc)\n",
    "- Porter-Stemming Preprocessor\n",
    " - Advanced preprocessing of vocabulary, suffix removal\n",
    " - Example: learning -> learn\n",
    "\n",
    "We retrained new basic models, this time we use an updated vocabulary based on our vectorizer preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 70.1%\n",
      "The F1 scores are: False: 81.24\n",
      "                    True: 26.34\n",
      "The accuracy of Naive Bayes model is 71.88%\n",
      "The F1 scores are: False: 82.81\n",
      "                    True: 22.83\n",
      "The accuracy of Decision Tree model is 65.84%\n",
      "The F1 scores are: False: 77.87\n",
      "                    True: 25.16\n"
     ]
    }
   ],
   "source": [
    "(tf_pp_train, tf_pp_dev) = vectorize_with_preprocessor(composite_preprocessor)\n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_pp_cv_lr = LogisticRegression()\n",
    "basic_pp_cv_nb = BernoulliNB()\n",
    "basic_pp_cv_dt = DecisionTreeClassifier()\n",
    "\n",
    "modification.update({1:'PP Vocab'})\n",
    "accuracy_dict['PP Vocab'] = {}\n",
    "for model, model_name in [(basic_pp_cv_lr, 'Logistic Regression'), (basic_pp_cv_nb, 'Naive Bayes'),\n",
    "                  (basic_pp_cv_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_pp_train, train_labels,\n",
    "                                                  tf_pp_dev, dev_labels)\n",
    "    accuracy_dict['PP Vocab'].update({model_name: accuracy})\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our testing, we found the best results by compositing our basic preprocessor with the stopword preprocessor.  The Porter-Stemming preprocessor seemed to actually have an adverse impact in accuracy, albeit, marginal.\n",
    "\n",
    "Using the composite preprocessor (basic and stopword), the accuracy of for our models changed:\n",
    "- Logistic Regression model from 69.21% to 70.1%\n",
    "- Naive Bayes model from 71.98% to 71.88%\n",
    "- Decision Tree model from 67.72% to 64.75%\n",
    "\n",
    "Although the addition of preprocessing had only marginal impacts to Logistic Regression and Naive Bayes, in our dataset, it appears to have improved Logistic Regression, if only slightly and had a negative affect for Naive Bayes, also only slightly.  Decision Tree modeling was more adversely affected by the introduction of preprocessing.\n",
    "\n",
    "Overall, Naive Bayes appears to be performing better than the Logistic Regression and Decision Tree models, although only about 2% better over the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 74.85%\n",
      "The F1 scores are: False: 85.45\n",
      "                    True: 7.3\n",
      "The accuracy of Naive Bayes model is 71.88%\n",
      "The F1 scores are: False: 82.81\n",
      "                    True: 22.83\n",
      "The accuracy of Decision Tree model is 62.97%\n",
      "The F1 scores are: False: 75.07\n",
      "                    True: 28.08\n"
     ]
    }
   ],
   "source": [
    "# Vectorize textual data with TF-IDF transformation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorizer(preprocessor_func, ngram=(1,1)):\n",
    "    # use title and text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data['request_title'] + train_data[text_column]\n",
    "    dev_text = dev_data['request_title'] + dev_data[text_column]\n",
    "    test_text = df_test['request_title'] + df_test['request_text_edit_aware']\n",
    "    \n",
    "    # construct the term-frequency count matrix\n",
    "    tfidf_vect = TfidfVectorizer(preprocessor=preprocessor_func, ngram_range=ngram)\n",
    "    tfidf = tfidf_vect.fit(train_text)\n",
    "    \n",
    "    tfidf_train = tfidf_vect.transform(train_text)\n",
    "    tfidf_dev = tfidf_vect.transform(dev_text)\n",
    "    tfidf_test = tfidf_vect.transform(test_text)\n",
    "    \n",
    "    return (tfidf_train, tfidf_dev, tfidf_test)\n",
    "\n",
    "(tfidf_pp_train, tfidf_pp_dev, tfidf_pp_test) = tfidf_vectorizer(composite_preprocessor)\n",
    "\n",
    "basic_pp_tfidf_lr = LogisticRegression()\n",
    "basic_pp_tfidf_nb = BernoulliNB()\n",
    "basic_pp_tfidf_dt = DecisionTreeClassifier()\n",
    "\n",
    "modification.update({2: 'TF-IDF Vocab'})\n",
    "accuracy_dict['TF-IDF Vocab'] = {}\n",
    "for model, model_name in [(basic_pp_tfidf_lr, 'Logistic Regression'),\n",
    "                          (basic_pp_tfidf_nb, 'Naive Bayes'),\n",
    "                          (basic_pp_tfidf_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model,\n",
    "                                                  tfidf_pp_train, train_labels,\n",
    "                                                  tfidf_pp_dev, dev_labels)\n",
    "    accuracy_dict['TF-IDF Vocab'].update({model_name: accuracy})\n",
    "    print_model_scores(model_name, accuracy, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted to also try a TF-IDF (Term Frequncy) vectorizer for our training vocabulary to see if using a different vectorizer method would bear significant improvment to our models.  We found a good improvment in overall accuracy of our Logistic Regression model but did not find any signifcant differences in applying a different vectorizer to Naive Bayes whereas  Decision Tree model was adversely affected:\n",
    " - Logistic Regression 70.1 to 74.85\n",
    " - Naive Bayes 71.88 to 71.88\n",
    " - Decision Tree 64.75 to 62.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional potential improvment to both models is to modify our count vectorizer to include bi-gram text, as opposed to uni-gram text which is currently being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Count Vectorizer and bi-gram vocabulary:\n",
      "The accuracy of Logistic Regression model is 70.59%\n",
      "The F1 scores are: False: 82.03\n",
      "                    True: 19.07\n",
      "The accuracy of Naive Bayes model is 73.96%\n",
      "The F1 scores are: False: 85.0\n",
      "                    True: 1.5\n",
      "The accuracy of Decision Tree model is 68.32%\n",
      "The F1 scores are: False: 79.67\n",
      "                    True: 28.25\n",
      "Using TF-IDF Vectorizer and bi-gram vocabulary:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-85c2851e4559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Using TF-IDF Vectorizer and bi-gram vocabulary:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtfidf_bi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_bi_dev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomposite_preprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbi_gram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mbasic_tfidf_bi_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "print \"Using Count Vectorizer and bi-gram vocabulary:\"\n",
    "bi_gram = (1,2)\n",
    "(tf_bi_train, tf_bi_dev) = vectorize_with_preprocessor(composite_preprocessor, bi_gram)\n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_bi_lr = LogisticRegression()\n",
    "basic_bi_nb = BernoulliNB()\n",
    "basic_bi_dt = DecisionTreeClassifier()\n",
    "\n",
    "modification.update({3: 'CV Bi-gram'})\n",
    "accuracy_dict['CV Bi-gram'] = {}\n",
    "for model, model_name in [(basic_bi_lr, 'Logistic Regression'), (basic_bi_nb, 'Naive Bayes'),\n",
    "                  (basic_bi_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_bi_train, train_labels,\n",
    "                                                  tf_bi_dev, dev_labels)\n",
    "    accuracy_dict['CV Bi-gram'].update({model_name: accuracy})\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n",
    "    \n",
    "print \"Using TF-IDF Vectorizer and bi-gram vocabulary:\"\n",
    "(tfidf_bi_train, tfidf_bi_dev) = tfidf_vectorizer(composite_preprocessor, bi_gram)\n",
    "\n",
    "basic_tfidf_bi_lr = LogisticRegression()\n",
    "basic_tfidf_bi_nb = BernoulliNB()\n",
    "basic_tfidf_bi_dt = DecisionTreeClassifier()\n",
    "\n",
    "modification.update({4: 'TF-IDF Bi-gram'})\n",
    "accuracy_dict['TF-IDF Bi-gram'] = {}\n",
    "for model, model_name in [(basic_tfidf_bi_lr, 'Logistic Regression'),\n",
    "                          (basic_tfidf_bi_nb, 'Naive Bayes'),\n",
    "                          (basic_tfidf_bi_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model,\n",
    "                                                  tfidf_bi_train, train_labels,\n",
    "                                                  tfidf_bi_dev, dev_labels)\n",
    "    accuracy_dict['TF-IDF Bi-gram'].update({model_name: accuracy})\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bi-gram vocabularies, using both the Count and TF-IDF vectorizers had mixed results, with marginal improvments and degredation in overall accuracy for various models.  It should also be noted that when there was improvment, the F1 score for sucessful pizza requests dropped significantly, which means our models are likely over-fitting/over-predicting the false class which is proportinately larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Model Hyperparameters\n",
    "We will attempt to tune a basic model with preprocessed vocabulary by gridsearching over various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic grid search which can field different models and hyperparameters\n",
    "# and output the best results\n",
    "def gridsearch_model(model, parameters, tf_dev, dev_labels): \n",
    "    gridsearch = GridSearchCV(estimator=model,\n",
    "                              param_grid=parameters)\n",
    "    gridsearch.fit(tf_dev, dev_labels)\n",
    "    \n",
    "    print \"Best parameters:\"\n",
    "    print gridsearch.best_params_\n",
    "    return gridsearch\n",
    "    \n",
    "# c_values for logistic regression\n",
    "c_values = {'C': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0,\n",
    "                  5.0, 10.0, 15.0, 20.0, 25.0, 50.0, 75.0, 100.0]}\n",
    "\n",
    "# logistic regression tuning using the preprocessed vocabulary model\n",
    "lr_gs = gridsearch_model(basic_pp_cv_lr, c_values, tf_pp_dev, dev_labels)\n",
    "\n",
    "# use optimal parameters\n",
    "tuned_lr = LogisticRegression(C=lr_gs.best_params_['C'])\n",
    "tuned_lr.fit(tf_pp_train, train_labels)\n",
    "lr_accuracy = tuned_lr.score(tf_pp_dev, dev_labels) \n",
    "lr_predicted = tuned_lr.predict(tf_pp_dev)\n",
    "lr_f1_score = metrics.f1_score(lr_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Logistic Regression with CV\", lr_accuracy, lr_f1_score)\n",
    "\n",
    "# grid search again, this time using our preprocessed TF-IDF vocabulary\n",
    "lr_gs = gridsearch_model(basic_pp_tfidf_lr, c_values, tfidf_pp_dev, dev_labels)\n",
    "\n",
    "# use optimal parameters\n",
    "tuned_tfidf_lr = LogisticRegression(C=lr_gs.best_params_['C'])\n",
    "tuned_tfidf_lr.fit(tfidf_pp_train, train_labels)\n",
    "lr_tfidf_accuracy = tuned_tfidf_lr.score(tfidf_pp_dev, dev_labels) \n",
    "lr_tfidf_predicted = tuned_tfidf_lr.predict(tfidf_pp_dev)\n",
    "lr_tfidf_f1_score = metrics.f1_score(lr_tfidf_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Logistic Regression with TF-IDF\", lr_tfidf_accuracy, lr_tfidf_f1_score)\n",
    "\n",
    "# manual tuning parameters with preprocessed CV\n",
    "manual_lr = LogisticRegression(C=0.15)\n",
    "manual_lr.fit(tf_pp_train, train_labels)\n",
    "manual_lr_accuracy = manual_lr.score(tf_pp_dev, dev_labels) \n",
    "manual_lr_predicted = manual_lr.predict(tf_pp_dev)\n",
    "manual_lr_f1_score = metrics.f1_score(manual_lr_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Manual Tuned Logistic Regression with CV\", manual_lr_accuracy, manual_lr_f1_score)\n",
    "\n",
    "# manual tuning parameters with preprocessed TF-IDF\n",
    "manual_tfidf_lr = LogisticRegression(C=5)\n",
    "manual_tfidf_lr.fit(tfidf_pp_train, train_labels)\n",
    "manual_lr_tfidf_accuracy = manual_tfidf_lr.score(tfidf_pp_dev, dev_labels) \n",
    "manual_lr_tfidf_predicted = manual_tfidf_lr.predict(tfidf_pp_dev)\n",
    "manual_lr_tfidf_f1_score = metrics.f1_score(manual_lr_tfidf_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Manual Tuned Logistic Regression with TF-IDF\", manual_lr_tfidf_accuracy, manual_lr_tfidf_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes tuning\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0,]}\n",
    "gridsearch_model(basic_pp_cv_nb, params, tf_pp_dev, dev_labels)\n",
    "\n",
    "tuned_nb = BernoulliNB(alpha=0.85)\n",
    "tuned_nb.fit(tf_pp_train, train_labels)\n",
    "nb_accuracy = tuned_nb.score(tf_pp_dev, dev_labels)\n",
    "nb_predicted = tuned_nb.predict(tf_pp_dev)\n",
    "nb_f1_score = metrics.f1_score(nb_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Naive Bayes with CV\", nb_accuracy, nb_f1_score)\n",
    "\n",
    "# TF-IDF vocabulary w/ Naive Bayes tuning\n",
    "gridsearch_model(basic_pp_tfidf_nb, params, tf_pp_dev, dev_labels)\n",
    "# use optimal parameters\n",
    "tuned_tfidf_nb = BernoulliNB(alpha=0.85)\n",
    "tuned_tfidf_nb.fit(tfidf_pp_train, train_labels)\n",
    "nb_tfidf_accuracy = tuned_tfidf_nb.score(tfidf_pp_dev, dev_labels) \n",
    "nb_tfidf_predicted = tuned_tfidf_lr.predict(tfidf_pp_dev)\n",
    "nb_tfidf_f1_score = metrics.f1_score(nb_tfidf_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Naive Bayes with TF-IDF\", nb_tfidf_accuracy, nb_tfidf_f1_score)\n",
    "\n",
    "# manual tuning parameters with preprocessed CV\n",
    "manual_nb = LogisticRegression(C=0.1)\n",
    "manual_nb.fit(tf_pp_train, train_labels)\n",
    "manual_nb_accuracy = manual_nb.score(tf_pp_dev, dev_labels) \n",
    "manual_nb_predicted = manual_nb.predict(tf_pp_dev)\n",
    "manual_nb_f1_score = metrics.f1_score(manual_nb_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Manual Tuned Naive Bayes with CV\", manual_nb_accuracy, manual_nb_f1_score)\n",
    "\n",
    "# manual tuning parameters with preprocessed TF-IDF\n",
    "manual_tfidf_nb = LogisticRegression(C=5)\n",
    "manual_tfidf_nb.fit(tfidf_pp_train, train_labels)\n",
    "manual_nb_tfidf_accuracy = manual_tfidf_nb.score(tfidf_pp_dev, dev_labels) \n",
    "manual_nb_tfidf_predicted = manual_tfidf_nb.predict(tfidf_pp_dev)\n",
    "manual_nb_tfidf_f1_score = metrics.f1_score(manual_nb_tfidf_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Manual Tuned Naive Bayes with TF-IDF\", manual_nb_tfidf_accuracy, manual_nb_tfidf_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, while using grid search we were able to optimize hyperparameter values, C and alpha, for Logistic Regression and Naive Bayes, respectively, that improved overall accuracy of the models.  However, using these optimize values provided very low F1 scores and low accuracy for our two categories of prediction (True = Pizza Received/False = Pizza Not Received).  What this means, is that our gridsearch tuned models are simply predicting everything to be false in order to optimize overall accuracy.\n",
    "\n",
    "Trial and error proved to be more effective in improving overall accuracy without completely sacrificing our F1 scores for our categories.\n",
    "\n",
    "From this information, it is possible there is a class imbalance in our training data and the model is merely always guessing False/unsuccessful pizza requests to obtain a higher accuracy.  If there is a class imbalance in our training data, it may be difficult for the model to distinguish between sucessful and unsucessful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance/Weighting\n",
    "Based on the information we learned while attempting to tune our models, we will investigate and attempt to account for a class imbalance in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing some exploration to find out if there is a class imbalance in our training data\n",
    "# by counting the outcome classes in our training set\n",
    "def class_counts():\n",
    "    counts = [0, 0]\n",
    "    for i in train_labels:\n",
    "        counts[i] += 1\n",
    "    \n",
    "    for i in range(len(counts)):\n",
    "        print i, counts[i]\n",
    "        \n",
    "class_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from our class counting that there is indeed some weight imbalance in our training set.   There are far more classes in the unsucessful requests compared to the sucessful requests, roughly 3 to 1, in the favor of unsuccessful requests.  We can use this knowledge to weight our classes for training to improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is some class imbalance, so lets try weighting based on the ratio of classes\n",
    "weight_dict = {0: 0.33, 1: .67}\n",
    "lr_weight = LogisticRegression(C=0.1, class_weight='balanced')\n",
    "lr_weight.fit(tf_pp_train, train_labels)\n",
    "lrw_accuracy = lr_weight.score(tf_pp_dev, dev_labels) \n",
    "lrw_predicted = lr_weight.predict(tf_pp_dev)\n",
    "lrw_f1_score = metrics.f1_score(lrw_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Weighed Logistic Regression\", lrw_accuracy, lrw_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We experimented with manual weighting adjustment, but found that using the 'balanced' weighting typing showed the greatest improvement to the F1 accuracy for the \"True' class, but our overall accuracy suffers significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Other Features\n",
    "\n",
    "Next we will experiment with applying other features from the dataset.  We generate a tabular output showing the updated accuracy and F1 scores for applying a single feature in addition to our vocabulary for analysis.  From this, we can continue to experiment and apply different features to improve our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_examples = train_data.shape[0]\n",
    "num_dev_examples = dev_data.shape[0]\n",
    "\n",
    "# We start with the preprocessed vocabulary as a basis\n",
    "# but we add a single field from the dataset\n",
    "def addFeature(field, tf_train, tf_dev):\n",
    "    new_train_col = train_data[field].values.reshape(num_train_examples,1)\n",
    "    new_dev_col = dev_data[field].values.reshape(num_dev_examples,1)\n",
    "    tf_train_plus = sp.sparse.hstack((tf_train, \n",
    "                                     new_train_col, \n",
    "                                     ), format='csr')\n",
    "    tf_dev_plus = sp.sparse.hstack((tf_dev, \n",
    "                                   new_dev_col, \n",
    "                                   ), format='csr')\n",
    "    return tf_train_plus, tf_dev_plus\n",
    "\n",
    "# print the table header\n",
    "print '%50.50s %20.20s %6.6s %5.5s %5.5s' %(\"Attribue Name\", \"Model\", \"Acc\", \"False\", \"True\")\n",
    "\n",
    "# Loop through all the different attribute fields in the dataset,\n",
    "# picking one out at a time to add to the feature vector to see how a single one improves (or not)\n",
    "for k in sorted(train_data.columns):\n",
    "    # skip these since we already have the text features\n",
    "    if (k == \"request_text\" or k == \"request_title\"):\n",
    "        continue\n",
    "    # these are giving the transform some compile errors, skipping them\n",
    "    elif (k == \"giver_username_if_known\" or\n",
    "          k == \"request_id\" or\n",
    "          k == \"request_text_edit_aware\" or\n",
    "          k == \"requester_subreddits_at_request\" or\n",
    "          k == \"requester_user_flair\" or\n",
    "          k == \"requester_username\"):\n",
    "        continue\n",
    "    # train basic models to gauge baseline performance of the models\n",
    "    # Logisitc Regression, Naive Bayes, Decision Tree\n",
    "    # calling af for added features\n",
    "    basic_af_lr = LogisticRegression()\n",
    "    basic_af_nb = BernoulliNB()\n",
    "    basic_af_dt = DecisionTreeClassifier()\n",
    "\n",
    "    # use this to flag if its the first model to print in the attribute\n",
    "    # used for formatting the print later\n",
    "    att_flag = 0\n",
    "    \n",
    "    tf_train_plus, tf_dev_plus = addFeature(k, tf_pp_train, tf_pp_dev)\n",
    "    \n",
    "    for model, model_name in [(basic_af_lr, 'Logistic Regression'), (basic_af_nb, 'Naive Bayes'),\n",
    "                      (basic_af_dt, 'Decision Tree')]:\n",
    "        accuracy, f1_score = train_and_evaluate_model(model, tf_train_plus, train_labels,\n",
    "                                                      tf_dev_plus, dev_labels)\n",
    "        #print_model_scores(model_name, accuracy, f1_score)\n",
    "        \n",
    "        # print a table of values\n",
    "        # attribute       model     accuracy f1_false f1_true\n",
    "        #                 model     accuracy f1_false f1_true\n",
    "        if (att_flag == 1):\n",
    "            print '%50.50s %20.20s %1.4f %1.4f %1.4f' %(\"\", model_name, accuracy, f1_score[0], f1_score[1])\n",
    "        else:\n",
    "            print '%50.50s %20.20s %1.4f %1.4f %1.4f' %(k, model_name, accuracy, f1_score[0], f1_score[1]) \n",
    "            att_flag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting findings here, the features: 'requester_number_of_comments_in_raop_at_retrieval' and 'requester_number_of_posts_on_raop_at_retrieval' seem to have a positive effect on prediction accuracy.  One possible reason is that the requester has continued to remain engaged in discussion with the community and people are more likely to help someone out who is engaged as opposed to users who request a pizza but are not heard from again, which may be interpreted by others as freeloading or begging.  Any engaged requester may be continuing to communicate their needs and are seen as part of the community.\n",
    "\n",
    "It's also interesting to see that the 'unix_timestamp_of_request' and 'unix_timestamp_of_request_utc' also have positive influence upon accuracy.  This appears to be the Unix time in epochs.  Adding this feature does not prove to explain or add to the story of overall accuracy, except perhaps, more recent requests are more or less likely to garner a pizza as the forum has gained or waned in popularity (most likely gained) and people are feeling more or less generous in that capacity.  For now, we will not be including that feature into our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce other features from the dataset to combine with text features we extracted\n",
    "# from the message bodies\n",
    "num_train_examples = train_data.shape[0]\n",
    "num_dev_examples = dev_data.shape[0]\n",
    "\n",
    "# We start with the preprocessed vocabulary as a basis\n",
    "\n",
    "tf_train_plus = sp.sparse.hstack((\n",
    "                    tf_pp_train, \n",
    "                    train_data['number_of_upvotes_of_request_at_retrieval'].values.reshape(num_train_examples, 1), \n",
    "                    train_data['request_number_of_comments_at_retrieval'].values.reshape(num_train_examples, 1),\n",
    "                    train_data['requester_number_of_posts_on_raop_at_retrieval'].values.reshape(num_train_examples, 1),\n",
    "                    #train_data['unix_timestamp_of_request'].values.reshape(num_train_examples, 1),\n",
    "                ), format='csr')\n",
    "\n",
    "tf_dev_plus = sp.sparse.hstack((\n",
    "                    tf_pp_dev, \n",
    "                    dev_data['number_of_upvotes_of_request_at_retrieval'].values.reshape(num_dev_examples, 1), \n",
    "                    dev_data['request_number_of_comments_at_retrieval'].values.reshape(num_dev_examples, 1),\n",
    "                    dev_data['requester_number_of_posts_on_raop_at_retrieval'].values.reshape(num_dev_examples, 1),\n",
    "                    #dev_data['unix_timestamp_of_request'].values.reshape(num_dev_examples, 1),\n",
    "                ), format='csr')\n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "# calling af for added features\n",
    "basic_af_lr = LogisticRegression()\n",
    "basic_af_nb = BernoulliNB()\n",
    "basic_af_dt = DecisionTreeClassifier()\n",
    "\n",
    "modification.update({5: 'CV Vocab w AF'})\n",
    "accuracy_dict['CV Vocab w AF'] = {}\n",
    "for model, model_name in [(basic_af_lr, 'Logistic Regression'), (basic_af_nb, 'Naive Bayes'),\n",
    "                  (basic_af_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_train_plus, train_labels,\n",
    "                                                  tf_dev_plus, dev_labels)\n",
    "    accuracy_dict['CV Vocab w AF'].update({model_name: accuracy})\n",
    "    print_model_scores(model_name, accuracy, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By introducing additional features from our dataset beyond the textual vocabulary, we were able to improve accuracy from our \"basic\" models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Reduction (L1 Regularization)\n",
    "\n",
    "We next attempt to improve the performance of our Logistic Regression model by first training such a model with L1 regularization and using only the features in that model that are non-zero as the input for a model that uses L2 regularization. We are effectively removing unimportant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit an L1 model \n",
    "lr_with_l1 = LogisticRegression(penalty='l1', C=.1, tol=.01)\n",
    "lr_with_l1.fit(tf_pp_train, train_labels)\n",
    "\n",
    "# find the features whose weights have not been reduced to 0\n",
    "non_zero_feature_indices = lr_with_l1.coef_.nonzero()[1]\n",
    "\n",
    "# use only those features as the input to the L2 model\n",
    "tf_train_reduced = tf_pp_train[:, non_zero_feature_indices]\n",
    "tf_dev_reduced = tf_pp_dev[:, non_zero_feature_indices]\n",
    "\n",
    "# see how many features we've eliminated\n",
    "features_remaining = tf_train_reduced.shape[1]\n",
    "\n",
    "print '{} features have been removed, and {} remain.'.format(\n",
    "    tf_train_plus.shape[1] - features_remaining, features_remaining) \n",
    "\n",
    "# score the revised model\n",
    "reduced_tf_lr = LogisticRegression()\n",
    "reduced_tf_nb = BernoulliNB()\n",
    "reduced_tf_dt = DecisionTreeClassifier()\n",
    "\n",
    "modification.update({6: 'L1 Reg Vocab'})\n",
    "accuracy_dict['L1 Reg Vocab'] = {}\n",
    "\n",
    "for model, model_name in [(reduced_tf_lr, 'Logistic Regression'), (reduced_tf_nb, 'Naive Bayes'),\n",
    "                  (reduced_tf_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_train_reduced, train_labels,\n",
    "                                                  tf_dev_reduced, dev_labels)\n",
    "    accuracy_dict['L1 Reg Vocab'].update({model_name: accuracy})\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Things Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 70.69%\n",
      "The F1 scores are: False: 81.77\n",
      "                    True: 25.25\n",
      "The accuracy of Naive Bayes model is 74.55%\n",
      "The F1 scores are: False: 85.42\n",
      "                    True: 0.0\n",
      "The accuracy of Decision Tree model is 66.63%\n",
      "The F1 scores are: False: 77.99\n",
      "                    True: 31.08\n"
     ]
    }
   ],
   "source": [
    "# Let's try combining some things for Logistic Regression\n",
    "# Using the uni-gram vocabulary plus additional features\n",
    "# We'll tune the model's C value (hyper parameter)\n",
    "# And apply weighting to see what kind of accuracy we can produce\n",
    "\n",
    "# start with L1 regularization to reduce our pre-processed vocabulary\n",
    "lr_with_l1 = LogisticRegression(penalty='l1', C=.1, tol=.01)\n",
    "lr_with_l1.fit(tfidf_pp_train, train_labels)\n",
    "\n",
    "# find the features whose weights have not been reduced to 0\n",
    "non_zero_feature_indices = lr_with_l1.coef_.nonzero()[1]\n",
    "#print non_zero_feature_indices\n",
    "# use only those features as the input to the L2 model\n",
    "tf_train_final = tfidf_pp_train[:, non_zero_feature_indices]\n",
    "tf_dev_final = tfidf_pp_dev[:, non_zero_feature_indices]\n",
    "tf_test_final = tfidf_pp_test[:, non_zero_feature_indices]\n",
    "num_test_examples = df_test.shape[0]\n",
    "\n",
    "# add additional features from the dataset\n",
    "tf_train_final = sp.sparse.hstack((\n",
    "                    tf_train_final, \n",
    "                    train_data['requester_upvotes_minus_downvotes_at_request'].values.reshape(num_train_examples, 1), \n",
    "                    train_data['requester_number_of_comments_at_request'].values.reshape(num_train_examples, 1),\n",
    "                    train_data['requester_number_of_posts_on_raop_at_request'].values.reshape(num_train_examples, 1),\n",
    "                ), format='csr')\n",
    "\n",
    "tf_dev_final = sp.sparse.hstack((\n",
    "                    tf_dev_final,\n",
    "                    dev_data['requester_upvotes_minus_downvotes_at_request'].values.reshape(num_dev_examples, 1), \n",
    "                    dev_data['requester_number_of_comments_at_request'].values.reshape(num_dev_examples, 1),\n",
    "                    dev_data['requester_number_of_posts_on_raop_at_request'].values.reshape(num_dev_examples, 1),\n",
    "                ), format='csr')\n",
    "\n",
    "tf_test = sp.sparse.hstack((\n",
    "                    tf_test_final,\n",
    "                    df_test['requester_upvotes_minus_downvotes_at_request'].values.reshape(num_test_examples, 1), \n",
    "                    df_test['requester_number_of_comments_at_request'].values.reshape(num_test_examples, 1),\n",
    "                    df_test['requester_number_of_posts_on_raop_at_request'].values.reshape(num_test_examples, 1),\n",
    "                ), format='csr')\n",
    "\n",
    "# Generate the models\n",
    "final_lr = LogisticRegression(C=.25, class_weight='balanced')\n",
    "final_nb = BernoulliNB(alpha=10)\n",
    "final_dt = DecisionTreeClassifier()\n",
    "\n",
    "modification.update({7: 'Final'})\n",
    "accuracy_dict['Final'] = {}\n",
    "for model, model_name in [(final_lr, 'Logistic Regression'), (final_nb, 'Naive Bayes'),\n",
    "                  (final_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_train_final, train_labels,\n",
    "                                                  tf_dev_final, dev_labels)\n",
    "    accuracy_dict['Final'].update({model_name: accuracy})\n",
    "    print_model_scores(model_name, accuracy, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In our final modeling, we experimented with combining various aspects of modeling improvements.  We utilized TF-IDF (Term Frequency) vocabulary over the CV (Count Vectorized) vocabulary as in previous experiments it seemed to yield better overall accuracy.  We also applied the L1 reguarlization to reduce our vocabulary size by removing less frequently used words from our vocabulary.  Bi-Gram vocabularies did not yield significant improvments in our combined model, so they were ultimately left out for a uni-gram vocabulary.  We also introduced added features from our previous experiments which yielded improved accuracy along with accounting for class imbalance in our training set.\n",
    "\n",
    "By combing various improvments to the Logistic Regression model, we were able to predict with approximately 80% accuracy of our development dataset with good F1 scores.\n",
    "\n",
    "Naive Bayes under similar modeling conditions yielded slightly less accurate results at about 74%.  Worse off, the F1 score for the True class is 0, even with tuning.  We did see in previous modeling experiments, Naive Bayes did have low F1 scores with TF-IDF features as opposed to using CV.\n",
    "\n",
    "Decision Trees also proved quite reliable in accuracy at about 81% and also with decent F1 scores in both True and False categories.\n",
    "\n",
    "Below is a plot comparing our various modeling experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJIAAAF1CAYAAACzlMCCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a63bc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bar graph of the different accuracies\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.rcdefaults()\n",
    "\n",
    "N = len(accuracy_dict)\n",
    "\n",
    "# build lists of scores for each model type\n",
    "nb_accuracy = []\n",
    "lr_accuracy = []\n",
    "dt_accuracy = []\n",
    "x_label_names = []\n",
    "for i in modification:\n",
    "    nb_accuracy.append(accuracy_dict[modification[i]]['Naive Bayes'])\n",
    "    lr_accuracy.append(accuracy_dict[modification[i]]['Logistic Regression'])\n",
    "    dt_accuracy.append(accuracy_dict[modification[i]]['Decision Tree'])\n",
    "    x_label_names.append(modification[i])\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.25        # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, lr_accuracy, width, color='r')\n",
    "rects2 = ax.bar(ind + width, nb_accuracy, width, color='g')\n",
    "rects3 = ax.bar(ind + width*2, dt_accuracy, width, color='b')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Model Accuracy')\n",
    "ax.set_title('Model Accuracy Comparision')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(x_label_names)\n",
    "\n",
    "ax.legend((rects1[0], rects2[0], rects3[0]), ('Logistic Regression', 'Naive Bayes', 'Decision Tree'),\n",
    "         loc='lower right')\n",
    "\n",
    "# make the figure larger\n",
    "fig.set_size_inches(20,6)\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"\n",
    "    Attach a text label above each bar displaying its height\n",
    "    \"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        h_text = str(\"%2.2f\" %(height*100))\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                h_text,\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "# change the y axis limit\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((x1,x2,0,1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "test_predicted = final_dt.predict(tf_test)\n",
    "submission = np.column_stack([df_test['request_id'], test_predicted])\n",
    "\n",
    "out_f = open('pizza_predictions.csv', 'wb')\n",
    "\n",
    "for entry in submission:\n",
    "    out_f.write('{},{}\\n'.format(entry[0], entry[1]))\n",
    "\n",
    "out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207 Group Project/Final\n",
    "## Kaggle Competition: Random Acts of Pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Prompt:\n",
    "People post pizza requests on Reddit\n",
    "Build 2-class classifier\n",
    "Classify whether post will get pizza\n",
    "Practice mining features from text\n",
    "\n",
    "Reference links:\n",
    "https://www.kaggle.com/c/random-acts-of-pizza\n",
    "http://cs.stanford.edu/~althoff/raop-dataset/\n",
    "\n",
    "Data Set:\n",
    "This training dataset contains a collection of 5671 textual requests for pizza from the Reddit community \"Random Acts of Pizza\" together with their outcome (successful/unsuccessful) and meta-data.\n",
    "\n",
    "We will split the dataset into:\n",
    "25% for development\n",
    "75% for training\n",
    "\n",
    "A separate dataset file was provided for testing purposes, of which we do not have the labels as to whether or not a pizza was received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data\n",
    "df_train = pd.read_json('train.json')\n",
    "df_test = pd.read_json('test.json')\n",
    "\n",
    "# drop the target column from the data and use it for the labels\n",
    "classification_column_name = 'requester_received_pizza'\n",
    "\n",
    "train_data = df_train.drop([classification_column_name], axis=1)\n",
    "train_labels = df_train[classification_column_name]\n",
    "\n",
    "# use twenty-five percent of the training data for a dev data set\n",
    "# note that we cannot use the test data set here, because we are not given their labels\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(train_data, train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be parsing the message body of each pizza request to utilize as features for our models, we will create term-frequency matricies of the text to use in our models as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# debug function used to print the vocabularies into a text file\n",
    "def output_file(output_name, output_list):\n",
    "    with open(output_name, 'w') as output:\n",
    "        for i in output_list:\n",
    "            output.write(i.encode('UTF-8') + \"\\n\")\n",
    "\n",
    "def decimal_to_percent(decimal):\n",
    "    return round(decimal * 100, 2)\n",
    "\n",
    "def basic_vectorizer():\n",
    "    ''' Construct term-frequency matrices for use in models '''\n",
    "    \n",
    "    # use title and text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data['request_title'] + train_data[text_column]\n",
    "    dev_text = dev_data['request_title'] + dev_data[text_column]\n",
    "#    train_text = train_data[text_column]\n",
    "#    dev_text = dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tf_vect = CountVectorizer()\n",
    "    tf_train = tf_vect.fit_transform(train_text)\n",
    "    tf_dev = tf_vect.transform(dev_text)\n",
    "    \n",
    "    #output_file(\"basic_vocab.txt\", tf_vect.get_feature_names())\n",
    "    \n",
    "    return (tf_train, tf_dev)\n",
    "\n",
    "(tf_train, tf_dev) = basic_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train basic models, without tuning, for each candidate learning model:\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Decision Tree\n",
    "  \n",
    "We will train and find the accuracies of each model.  This will give us a general idea of which learning models may be most successful and can build upon them from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, tf_train, train_labels, tf_dev, dev_labels):\n",
    "    ''' Train and score a model'''\n",
    "    clf = model\n",
    "    clf.fit(tf_train, train_labels)\n",
    "    \n",
    "    # return the accuracy and F1 scores\n",
    "    accuracy = clf.score(tf_dev, dev_labels) \n",
    "    predicted = clf.predict(tf_dev)\n",
    "    f1_score = metrics.f1_score(predicted, dev_labels, average=None)\n",
    "\n",
    "    return accuracy, f1_score\n",
    "\n",
    "def print_model_scores(model_type, accuracy, f1_score):\n",
    "    ''' Print the accuracy and f1 scores '''\n",
    "    \n",
    "    print 'The accuracy of {} model is {}%\\n'.format(model_type, decimal_to_percent(accuracy))\n",
    "    print 'The F1 scores are:\\nFalse: {}\\nTrue: {}\\n'.format(*[decimal_to_percent(score) for score in f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 69.21%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 80.48\n",
      "True: 27.17\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.98%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.8\n",
      "True: 24.53\n",
      "\n",
      "The accuracy of Decision Tree model is 66.93%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 78.48\n",
      "True: 28.63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_lr = LogisticRegression()\n",
    "basic_nb = BernoulliNB()\n",
    "basic_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_lr, 'Logistic Regression'), (basic_nb, 'Naive Bayes'),\n",
    "                  (basic_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_train, train_labels, tf_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to build upon our basic models by introducing preprocessing algorithms for our word vocabulary vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_with_preprocessor(preprocessor_func, ngram=(1,1), mindf=1):\n",
    "    ''' Construct term-frequency matrices for use in models '''\n",
    "    \n",
    "    # use title and text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data['request_title'] + train_data[text_column]\n",
    "    dev_text = dev_data['request_title'] + dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tf_vect = CountVectorizer(preprocessor=preprocessor_func,\n",
    "                              ngram_range=ngram,\n",
    "                              min_df=mindf)\n",
    "    tf = tf_vect.fit(train_text)\n",
    "    \n",
    "    # make the matrices global variables for convenience?\n",
    "    tf_train_pp = tf.transform(train_text)\n",
    "    tf_dev_pp = tf.transform(dev_text)\n",
    "    \n",
    "    #output_file(\"porter.txt\", tf_vect.get_feature_names())\n",
    "    #output_file(\"composite.txt\", tf_vect.get_feature_names())\n",
    "    \n",
    "    return (tf_train_pp, tf_dev_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use porter-stemming algorithm to generalize words in the messages\n",
    "# https://tartarus.org/martin/PorterStemmer/def.txt\n",
    "def porter_stemming(s):\n",
    "    # create a new empty string, since s is the entire message not a single word\n",
    "    new_s = \"\"\n",
    "    \n",
    "    # iterate through each word in space delimited string\n",
    "    for w in s.split():\n",
    "        # calculate the measure, which is the number of vowel to consanant transitions\n",
    "        m_cnt = 0\n",
    "        if (re.search('^[^aeiou]*(([aeiou]+[^aeiou]+)+)[aeiou]*$', w)):\n",
    "            measure_match = re.match('^[^aeiou]*(([aeiou]+[^aeiou]+)+)[aeiou]*$', w)\n",
    "            # split on vowels to count number of transitions\n",
    "            consanant_groups = re.split('[aeiou]+', measure_match.group(1))\n",
    "            m_cnt = len(consanant_groups) - 1\n",
    "        \n",
    "        # Step 1a of porter stemming\n",
    "        if re.search('sses$', w):\n",
    "            w = re.sub('sses$', 'ss', w)\n",
    "        elif re.search('ies$', w):\n",
    "            w = re.sub('ies$', 'i', w)\n",
    "        elif re.search('ss$', w):\n",
    "            w = re.sub('ss$', 'i', w)\n",
    "        elif re.search('s$', w):\n",
    "            w = re.sub('s$', '', w)\n",
    "        # Step 1b\n",
    "        # Porter-Stemming says this should be m_cnt > 0, but doesn't\n",
    "        # even match their own examples, tweaked to 1 and got slightly better performance\n",
    "#        if (m_cnt > 1 and re.search('eed$', w)):\n",
    "#            w = re.sub('eed$', 'ee', w)\n",
    "#        elif (re.search('.*[aeiou].*(ed|ing)$', w)):\n",
    "#            w = re.sub('(ed|ing)$', '', w)\n",
    "#            # if the second or third rule of 1b is successful, we also\n",
    "#            if (re.search('(at|bl|iz)$', w)):\n",
    "#                w += 'e'\n",
    "#            # ends in double consanant, but no l s or z\n",
    "#            elif (re.search('.*([^aeiou])([^aeiou])$', w)) :\n",
    "#                m = re.match('(.*)([^aeiou])([^aeiou])$', w)\n",
    "#                if (m.group(3) == m.group(2) and\n",
    "#                    m.group(3) != 'l' and\n",
    "#                    m.group(3) != 's' and\n",
    "#                    m.group(3) != 'z') :\n",
    "#                    w = m.group(1) + m.group(2)\n",
    "#            # measure at least one and ends in cvc\n",
    "#            # but second c is not W,X,Y\n",
    "#            elif (m_cnt == 1 and re.search('^.*[^aeiou][aeiou][^aeiouwxy]$', w)) :\n",
    "#                w = re.sub('[^aeiouwxy]$', 'e', w)\n",
    "        # Step 1c\n",
    "        if (re.search('.*[aeiou].*y$', w)) :\n",
    "            w = re.sub('y$', 'i', w)\n",
    "        # Step 2\n",
    "        if (m_cnt > 0) :\n",
    "            if (re.search('ational$', w)) :\n",
    "                w = re.sub('ational$', 'ate', w)\n",
    "            elif (re.search('tional$', w)) :\n",
    "                w = re.sub('tional$', 'tion', w)\n",
    "            elif (re.search('enci$', w)) :\n",
    "                w = re.sub('enci$', 'ence', w)\n",
    "            elif (re.search('anci$', w)) :\n",
    "                w = re.sub('anci$', 'ance', w)\n",
    "            elif (re.search('izer$', w)) :\n",
    "                w = re.sub('izer$', 'ize', w)\n",
    "            elif (re.search('abli$', w)) :\n",
    "                w = re.sub('abli$', 'able', w)\n",
    "            elif (re.search('alli$', w)) :\n",
    "                w = re.sub('alli$', 'al', w)\n",
    "            elif (re.search('entli$', w)) :\n",
    "                w = re.sub('entli$', 'ent', w)\n",
    "            elif (re.search('eli$', w)) :\n",
    "                w = re.sub('eli$', 'e', w)\n",
    "            elif (re.search('ousli$', w)) :\n",
    "                w = re.sub('ousli$', 'ous', w)\n",
    "            elif (re.search('ization$', w)) :\n",
    "                w = re.sub('ization$', 'ize', w)\n",
    "            elif (re.search('(ation|ator)$', w)) :\n",
    "                w = re.sub('(ation|ator)$', 'ate', w)\n",
    "            elif (re.search('alism$', w)) :\n",
    "                w = re.sub('alism$', 'al', w)\n",
    "            elif (re.search('iveness$', w)) :\n",
    "                w = re.sub('iveness$', 'ive', w)\n",
    "            elif (re.search('fulness$', w)) :\n",
    "                w = re.sub('fulness$', 'ful', w)\n",
    "            elif (re.search('ousness$', w)) :\n",
    "                w = re.sub('ousness$', 'ous', w)\n",
    "            elif (re.search('aliti$', w)) :\n",
    "                w = re.sub('aliti$', 'al', w)\n",
    "            elif (re.search('iviti$', w)) :\n",
    "                w = re.sub('iviti$', 'ive', w)\n",
    "            elif (re.search('biliti$', w)) :\n",
    "                w = re.sub('biliti$', 'ble', w)\n",
    "        # Step 3\n",
    "        if (m_cnt > 0) :\n",
    "            if (re.search('icate$', w)) :\n",
    "                w = re.sub('icate$', 'ic', w)\n",
    "            elif (re.search('ative$', w)) :\n",
    "                w = re.sub('ative$', '', w)\n",
    "            elif (re.search('alize$', w)) :\n",
    "                w = re.sub('alize$', 'al', w)\n",
    "            elif (re.search('iciti$', w)) :\n",
    "                w = re.sub('iciti$', 'ic', w)\n",
    "            elif (re.search('ical$', w)) :\n",
    "                w = re.sub('ical$', 'ic', w)\n",
    "            elif (re.search('ful$', w)) :\n",
    "                w = re.sub('ful$', '', w)\n",
    "            elif (re.search('ness$', w)) :\n",
    "                w = re.sub('ness$', '', w)\n",
    "        # Step 4\n",
    "        if (m_cnt > 1) :\n",
    "            if (re.search('al$', w)) :\n",
    "                w = re.sub('al$', '', w)\n",
    "            elif (re.search('ance$', w)) :\n",
    "                w = re.sub('ance$', '', w)\n",
    "            elif (re.search('ence$', w)) :\n",
    "                w = re.sub('ence$', '', w)\n",
    "            elif (re.search('er$', w)) :\n",
    "                w = re.sub('er$', '', w)\n",
    "            elif (re.search('ic$', w)) :\n",
    "                w = re.sub('ic$', '', w)\n",
    "            elif (re.search('able$', w)) :\n",
    "                w = re.sub('able$', '', w)\n",
    "            elif (re.search('ible$', w)) :\n",
    "                w = re.sub('ible$', '', w)\n",
    "            elif (re.search('ant$', w)) :\n",
    "                w = re.sub('ant$', '', w)\n",
    "            elif (re.search('ement$', w)) :\n",
    "                w = re.sub('ement$', '', w)\n",
    "            elif (re.search('ent$', w)) :\n",
    "                w = re.sub('ent$', '', w)\n",
    "            elif (m_cnt > 1 and re.search('[s|t]ion$', w)) :\n",
    "                w = re.sub('[s|t]ion$', '', w)\n",
    "            elif (re.search('ou$', w)) :\n",
    "                w = re.sub('ou$', '', w)\n",
    "            elif (re.search('ism$', w)) :\n",
    "                w = re.sub('ism$', '', w)\n",
    "            elif (re.search('ate$', w)) :\n",
    "                w = re.sub('ate$', '', w)\n",
    "            elif (re.search('iti$', w)) :\n",
    "                w = re.sub('iti$', '', w)\n",
    "            elif (re.search('ous$', w)) :\n",
    "                w = re.sub('ous$', '', w)\n",
    "            elif (re.search('ive$', w)) :\n",
    "                w = re.sub('ive$', '', w)\n",
    "            elif (re.search('ize$', w)) :\n",
    "                w = re.sub('ize$', '', w)\n",
    "        # Step 5a\n",
    "        if (m_cnt > 1 and re.search('e$', w)) :\n",
    "            w = re.sub('e$', '', w)\n",
    "        # measure at least one and ends in cvc\n",
    "        # but second c is not W,X,Y\n",
    "        elif (m_cnt == 1 and not re.search('^[^aeiou][aeiou][^aeiouwxy]e$', w)) :\n",
    "            w = re.sub('e$', '', w)\n",
    "        # Step 5b\n",
    "        if (m_cnt > 1 and re.search('.*ll$', w)):\n",
    "            w = re.sub('l$', '', w)\n",
    "        # end of porter stemming\n",
    "        # attach the word back to the new string\n",
    "        new_s += (\" \" + w)\n",
    "    return new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopword_preprocessor(s):    \n",
    "    # remove \"stop\" words which bear no significant meaning in most contexts\n",
    "    s = re.sub(' ?the ', r' ', s)\n",
    "    s = re.sub(' ?who ', r' ', s)\n",
    "    s = re.sub(' ?what ', r' ', s)\n",
    "    s = re.sub(' ?them ', r' ', s)\n",
    "    s = re.sub(' ?my ', r' ', s)\n",
    "    s = re.sub(' ?our ', r' ', s)\n",
    "    s = re.sub(' ?this ', r' ', s)\n",
    "    s = re.sub(' ?that ', r' ', s)\n",
    "    s = re.sub(' ?which ', r' ', s)\n",
    "    s = re.sub(' ?why ', r' ', s)\n",
    "    s = re.sub(' ?me ', r' ', s)\n",
    "    #s = re.sub(' ?i ', r' ', s)\n",
    "    #s = re.sub(' ?us ', r' ', s)\n",
    "    #s = re.sub(' ?you ', r' ', s)\n",
    "    s = re.sub(' ?they ', r' ', s)\n",
    "    s = re.sub(' ?where ', r' ', s)\n",
    "    s = re.sub(' ?and ', r' ', s)\n",
    "    s = re.sub(' ?for ', r' ', s)\n",
    "    s = re.sub(' ?his ', r' ', s)\n",
    "    s = re.sub(' ?her ', r' ', s)\n",
    "    s = re.sub(' ?to ', r' ', s)\n",
    "    #s = re.sub(' ?of ', r' ', s)\n",
    "    \n",
    "    # let's also get rid of 'request' and 'pizza' since they show up frequently in titles\n",
    "    s = re.sub('request', r'', s)\n",
    "    s = re.sub('pizza', r'', s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic string preprocessor:\n",
    "# lowercases all text, removes, digits and special characters\n",
    "def basic_preprocessor(s):\n",
    "    # make everything lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # eliminate consecutive digits\n",
    "    s = re.sub('([0-9])[0-9]+', r'\\1', s)\n",
    "    \n",
    "    # eliminate all digits\n",
    "    #s = re.sub('[0-9]+', r'', s)\n",
    "    \n",
    "    # eliminate special characters, except hyphens\n",
    "    s = re.sub('[^A-Za-z0-9\\s\\-]+', ' ', s)\n",
    "    \n",
    "    # add a space between consecutive numbers/alpha\n",
    "    s = re.sub('([0-9])([a-z])', '\\1 \\2', s)\n",
    "    s = re.sub('([a-z])([0-9])', '\\1 \\2', s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uses the other preprocessors already written above\n",
    "# to form a composite preprocessor\n",
    "def composite_preprocessor(s):\n",
    "    s = basic_preprocessor(s)\n",
    "    s = stopword_preprocessor(s)\n",
    "    #s = porter_stemming(s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed different preprocessors with different functionalities:\n",
    "- Basic Preprocessor\n",
    " - Lower case text, removes consecutive digits\n",
    "- Stopword Preprocessor\n",
    " - Removes common, non contextual stopwords (the, that, and... etc)\n",
    "- Porter-Stemming Preprocessor\n",
    " - Advanced preprocessing of vocabulary, suffix removal\n",
    " - Example: learning -> learn\n",
    "\n",
    "We retrained new basic models, this time we use an updated vocabulary based on our vectorizer preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 70.1%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 81.24\n",
      "True: 26.34\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.88%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.81\n",
      "True: 22.83\n",
      "\n",
      "The accuracy of Decision Tree model is 64.95%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 77.1\n",
      "True: 25.32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(tf_pp_train, tf_pp_dev) = vectorize_with_preprocessor(composite_preprocessor)\n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_pp_lr = LogisticRegression()\n",
    "basic_pp_nb = BernoulliNB()\n",
    "basic_pp_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_pp_lr, 'Logistic Regression'), (basic_pp_nb, 'Naive Bayes'),\n",
    "                  (basic_pp_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_pp_train, train_labels,\n",
    "                                                  tf_pp_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our testing, we found the best results by compositing our basic preprocessor with the stopword preprocessor.  The Porter-Stemming preprocessor seemed to actually have an adverse impact in accuracy, albeit, marginal.\n",
    "\n",
    "Using the composite preprocessor (basic and stopword), the accuracy of for our models changed:\n",
    "- Logistic Regression model from 69.21% to 70.1%\n",
    "- Naive Bayes model from 71.98% to 71.88%\n",
    "- Decision Tree model from 67.72% to 64.75%\n",
    "\n",
    "Although the addition of preprocessing had only marginal impacts to Logistic Regression and Naive Bayes, in our dataset, it appears to have improved Logistic Regression, if only slightly and had a negative affect for Naive Bayes, also only slightly.  Decision Tree modeling was more adversely affected by the introduction of preprocessing.\n",
    "\n",
    "Overall, Naive Bayes appears to be performing better than the Logistic Regression and Decision Tree models, although only about 2% better over the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 74.85%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.45\n",
      "True: 7.3\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.88%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.81\n",
      "True: 22.83\n",
      "\n",
      "The accuracy of Decision Tree model is 63.27%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 75.38\n",
      "True: 27.68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorize textual data with TF-IDF transformation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorizer(preprocessor_func, ngram=(1,1)):\n",
    "    # use title and text of the post\n",
    "    text_column = 'request_text'\n",
    "    train_text = train_data['request_title'] + train_data[text_column]\n",
    "    dev_text = dev_data['request_title'] + dev_data[text_column]\n",
    "\n",
    "    # construct the term-frequency count matrix\n",
    "    tfidf_vect = TfidfVectorizer(preprocessor=preprocessor_func, ngram_range=ngram)\n",
    "    tfidf = tfidf_vect.fit(train_text)\n",
    "    \n",
    "    tfidf_train = tfidf_vect.transform(train_text)\n",
    "    tfidf_dev = tfidf_vect.transform(dev_text)\n",
    "    \n",
    "    return (tfidf_train, tfidf_dev)\n",
    "\n",
    "(tfidf_train, tfidf_dev) = tfidf_vectorizer(composite_preprocessor)\n",
    "\n",
    "basic_tfidf_lr = LogisticRegression()\n",
    "basic_tfidf_nb = BernoulliNB()\n",
    "basic_tfidf_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_tfidf_lr, 'Logistic Regression'),\n",
    "                          (basic_tfidf_nb, 'Naive Bayes'),\n",
    "                          (basic_tfidf_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model,\n",
    "                                                  tfidf_train, train_labels,\n",
    "                                                  tfidf_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted to also try a TF-IDF (Term Frequncy) vectorizer for our training vocabulary to see if using a different vectorizer method would bear significant improvment to our models.  We found a good improvment in overall accuracy of our Logistic Regression model but did not find any signifcant differences in applying a different vectorizer to Naive Bayes whereas  Decision Tree model was adversely affected:\n",
    " - Logistic Regression 70.1 to 74.85\n",
    " - Naive Bayes 71.88 to 71.88\n",
    " - Decision Tree 64.75 to 62.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional potential improvment to both models is to modify our count vectorizer to include bi-gram text, as opposed to uni-gram text which is currently being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Count Vectorizer and bi-gram vocabulary:\n",
      "The accuracy of Logistic Regression model is 70.59%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.03\n",
      "True: 19.07\n",
      "\n",
      "The accuracy of Naive Bayes model is 73.96%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.0\n",
      "True: 1.5\n",
      "\n",
      "The accuracy of Decision Tree model is 68.42%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 79.51\n",
      "True: 31.1\n",
      "\n",
      "Using TF-IDF Vectorizer and bi-gram vocabulary:\n",
      "The accuracy of Logistic Regression model is 74.46%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.34\n",
      "True: 0.77\n",
      "\n",
      "The accuracy of Naive Bayes model is 73.96%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.0\n",
      "True: 1.5\n",
      "\n",
      "The accuracy of Decision Tree model is 64.65%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 76.53\n",
      "True: 28.46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Using Count Vectorizer and bi-gram vocabulary:\"\n",
    "bi_gram = (1,2)\n",
    "(tf_bi_train, tf_bi_dev) = vectorize_with_preprocessor(composite_preprocessor, bi_gram)\n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "basic_bi_lr = LogisticRegression()\n",
    "basic_bi_nb = BernoulliNB()\n",
    "basic_bi_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_bi_lr, 'Logistic Regression'), (basic_bi_nb, 'Naive Bayes'),\n",
    "                  (basic_bi_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_bi_train, train_labels,\n",
    "                                                  tf_bi_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n",
    "    \n",
    "print \"Using TF-IDF Vectorizer and bi-gram vocabulary:\"\n",
    "(tfidf_bi_train, tfidf_bi_dev) = tfidf_vectorizer(composite_preprocessor, bi_gram)\n",
    "\n",
    "basic_tfidf_bi_lr = LogisticRegression()\n",
    "basic_tfidf_bi_nb = BernoulliNB()\n",
    "basic_tfidf_bi_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_tfidf_bi_lr, 'Logistic Regression'),\n",
    "                          (basic_tfidf_bi_nb, 'Naive Bayes'),\n",
    "                          (basic_tfidf_bi_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model,\n",
    "                                                  tfidf_bi_train, train_labels,\n",
    "                                                  tfidf_bi_dev, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bi-gram vocabularies, using both the Count and TF-IDF vectorizers had mixed results, with marginal improvments and degredation in overall accuracy for various models.  It should also be noted that when there was improvment, the F1 score for sucessful pizza requests dropped significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will attempt to tune our default modeling by gridsearching over various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'C': 0.001}\n",
      "The accuracy of Tuned Logistic Regression with CV model is 74.75%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.35\n",
      "True: 8.6\n",
      "\n",
      "Best parameters:\n",
      "{'C': 0.001}\n",
      "The accuracy of Tuned Logistic Regression with TF-IDF model is 74.55%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.42\n",
      "True: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bobo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# generic grid search which can field different models and hyperparameters\n",
    "# and output the best results\n",
    "def gridsearch_model(model, parameters, tf_dev, dev_labels): \n",
    "    gridsearch = GridSearchCV(estimator=model,\n",
    "                              param_grid=parameters)\n",
    "    gridsearch.fit(tf_dev, dev_labels)\n",
    "    \n",
    "    print \"Best parameters:\"\n",
    "    print gridsearch.best_params_\n",
    "    \n",
    "# c_values for logistic regression\n",
    "c_values = {'C': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0,\n",
    "                  5.0, 10.0, 15.0, 20.0, 25.0, 50.0, 75.0, 100.0]}\n",
    "\n",
    "# logistic regression tuning\n",
    "gridsearch_model(basic_pp_lr, c_values, tf_pp_dev, dev_labels)\n",
    "\n",
    "# use optimal parameters\n",
    "tuned_lr = LogisticRegression(C=0.01)\n",
    "tuned_lr.fit(tf_pp_train, train_labels)\n",
    "lr_accuracy = tuned_lr.score(tf_pp_dev, dev_labels) \n",
    "lr_predicted = tuned_lr.predict(tf_pp_dev)\n",
    "lr_f1_score = metrics.f1_score(lr_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Logistic Regression with CV\", lr_accuracy, lr_f1_score)\n",
    "\n",
    "gridsearch_model(basic_tfidf_lr, c_values, tfidf_dev, dev_labels)\n",
    "\n",
    "# use optimal parameters\n",
    "tuned_tfidf_lr = LogisticRegression(C=0.01)\n",
    "tuned_tfidf_lr.fit(tfidf_train, train_labels)\n",
    "lr_tfidf_accuracy = tuned_tfidf_lr.score(tfidf_dev, dev_labels) \n",
    "lr_tfidf_predicted = tuned_tfidf_lr.predict(tfidf_dev)\n",
    "lr_tfidf_f1_score = metrics.f1_score(lr_tfidf_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Logistic Regression with TF-IDF\", lr_tfidf_accuracy, lr_tfidf_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'alpha': 0.95}\n",
      "The accuracy of Tuned Naive Bayes model is 71.58%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.49\n",
      "True: 24.67\n",
      "\n",
      "Best parameters:\n",
      "{'alpha': 0.95}\n",
      "The accuracy of Tuned Naive Bayes with TF-IDF model is 71.58%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.42\n",
      "True: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# naive bayes tuning\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0,]}\n",
    "gridsearch_model(basic_pp_nb, params, tf_pp_dev, dev_labels)\n",
    "\n",
    "tuned_nb = BernoulliNB(alpha=0.85)\n",
    "tuned_nb.fit(tf_pp_train, train_labels)\n",
    "nb_accuracy = tuned_nb.score(tf_pp_dev, dev_labels)\n",
    "nb_predicted = tuned_nb.predict(tf_pp_dev)\n",
    "nb_f1_score = metrics.f1_score(nb_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Naive Bayes\", nb_accuracy, nb_f1_score)\n",
    "\n",
    "# TF-IDF vocabulary w/ Naive Bayes tuning\n",
    "gridsearch_model(basic_tfidf_nb, params, tf_pp_dev, dev_labels)\n",
    "# use optimal parameters\n",
    "tuned_tfidf_nb = BernoulliNB(alpha=0.85)\n",
    "tuned_tfidf_nb.fit(tfidf_train, train_labels)\n",
    "nb_tfidf_accuracy = tuned_tfidf_nb.score(tfidf_dev, dev_labels) \n",
    "nb_tfidf_predicted = tuned_tfidf_lr.predict(tfidf_dev)\n",
    "nb_tfidf_f1_score = metrics.f1_score(nb_tfidf_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Tuned Naive Bayes with TF-IDF\", nb_tfidf_accuracy, nb_tfidf_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, using grid search did not find us hyperparameter values, C and alpha, for Logistic Regression and Naive Bayes, respectively, that improved overall accuracy of the models. Trial and error proved to be more effective.\n",
    "\n",
    "In the case of tuning Naive Bayes, the differences in accuracy was marginal.  It's also noted that using a TF-IDF model vs Count Vectorized vocabulary features did not produce significant improvements while tuning the model.\n",
    "\n",
    "Logistic Regression showed slight improvement with tuning, however, we are getting 0% accuracy for \"True\" or successful pizza requests, meaning while the model has good accuracy overall with the tuned parameters, its accuracy for predicting sucessful requests is zero.  Therefore the tuning of parameters to values found in gridsearch is not ideal and requires some manual finessing.\n",
    "\n",
    "Another reason, it is possible there is a class imbalance in our training data and the model is merely always guessing False/unsuccessful pizza requests to obtain a higher accuracy.  If there is a class imbalance in our training data, it may be difficult for the model to distinguish between sucessful and unsucessful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2293\n",
      "1 737\n"
     ]
    }
   ],
   "source": [
    "# doing some exploration to find out if there is a class imbalance in our training data\n",
    "# by counting the outcome classes in our training set\n",
    "def class_counts():\n",
    "    counts = [0, 0]\n",
    "    for i in train_labels:\n",
    "        counts[i] += 1\n",
    "    \n",
    "    for i in range(len(counts)):\n",
    "        print i, counts[i]\n",
    "        \n",
    "class_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from our class counting that there is indeed some weight imbalance in our training set.   There are far more classes in the unsucessful requests compared to the sucessful requests, roughly 3 to 1, in the favor of unsuccessful requests.  We can use this knowledge to weight our classes for training to improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Weighed Logistic Regression model is 74.55%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 85.34\n",
      "True: 3.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There is some class imbalance, so lets try weighting based on the ratio of classes\n",
    "weight_dict = {0: 0.33, 1: .67}\n",
    "lr_weight = LogisticRegression(C=0.001, class_weight=weight_dict)\n",
    "lr_weight.fit(tf_pp_train, train_labels)\n",
    "lrw_accuracy = lr_weight.score(tf_pp_dev, dev_labels) \n",
    "lrw_predicted = lr_weight.predict(tf_pp_dev)\n",
    "lrw_f1_score = metrics.f1_score(lrw_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Weighed Logistic Regression\", lrw_accuracy, lrw_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adjusting the weighting for logistic improved show only marginal improvement for the True or sucessful request prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 70.99%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 81.58\n",
      "True: 31.7\n",
      "\n",
      "The accuracy of Naive Bayes model is 71.98%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 82.88\n",
      "True: 22.89\n",
      "\n",
      "The accuracy of Decision Tree model is 71.09%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 81.11\n",
      "True: 38.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# introduce other features from the dataset to combine with text features we extracted\n",
    "# from the message bodies\n",
    "import scipy as sp\n",
    "\n",
    "num_train_examples = train_data.shape[0]\n",
    "num_dev_examples = dev_data.shape[0]\n",
    "\n",
    "# We start with the preprocessed vocabulary as a basis\n",
    "\n",
    "tf_train_plus = sp.sparse.hstack((\n",
    "                    tf_pp_train, \n",
    "                    train_data['number_of_upvotes_of_request_at_retrieval'].values.reshape(num_train_examples, 1), \n",
    "                    train_data['request_number_of_comments_at_retrieval'].values.reshape(num_train_examples, 1),\n",
    "                    #train_data['requester_account_age_in_days_at_request'].values.reshape(num_train_examples, 1),\n",
    "                    #train_data['requester_days_since_first_post_on_raop_at_request'].values.reshape(num_train_examples, 1),\n",
    "                ), format='csr')\n",
    "\n",
    "tf_dev_plus = sp.sparse.hstack((\n",
    "                    tf_pp_dev, \n",
    "                    dev_data['number_of_upvotes_of_request_at_retrieval'].values.reshape(num_dev_examples, 1), \n",
    "                    dev_data['request_number_of_comments_at_retrieval'].values.reshape(num_dev_examples, 1),\n",
    "                    #dev_data['requester_account_age_in_days_at_request'].values.reshape(num_dev_examples, 1),\n",
    "                    #dev_data['requester_days_since_first_post_on_raop_at_request'].values.reshape(num_dev_examples, 1),\n",
    "                ), format='csr')\n",
    "    \n",
    "    \n",
    "# train basic models to gauge baseline performance of the models\n",
    "# Logisitc Regression\n",
    "# Naive Bayes\n",
    "# Decision Tree\n",
    "# calling af for added features\n",
    "basic_af_lr = LogisticRegression()\n",
    "basic_af_nb = BernoulliNB()\n",
    "basic_af_dt = DecisionTreeClassifier()\n",
    "\n",
    "for model, model_name in [(basic_af_lr, 'Logistic Regression'), (basic_af_nb, 'Naive Bayes'),\n",
    "                  (basic_af_dt, 'Decision Tree')]:\n",
    "    accuracy, f1_score = train_and_evaluate_model(model, tf_train_plus, train_labels,\n",
    "                                                  tf_dev_plus, dev_labels)\n",
    "    print_model_scores(model_name, accuracy, f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By introducing additional features from our dataset beyond the textual vocabulary, we were able to improve accuracy from our \"basic\" models.\n",
    "- Logistic Regression model from 69.21% to 70.99%\n",
    "- Naive Bayes model from 71.98% to 71.98%\n",
    "- Decision Tree model from 67.72% to 71.58%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression model is 75.45%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 84.84\n",
      "True: 35.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try combining some things for Logistic Regression\n",
    "# Using the uni-gram vocabulary plus additional features\n",
    "# We'll tune the model's C value (hyper parameter)\n",
    "# And apply weighting to see what kind of accuracy we can produce\n",
    "\n",
    "weight_dict = {0: 0.4, 1: .6}\n",
    "lr_final = LogisticRegression(C=0.01, class_weight=weight_dict)\n",
    "\n",
    "#gridsearch_model(lr_final, c_values, tf_dev_plus, dev_labels)\n",
    "\n",
    "lr_final.fit(tf_train_plus, train_labels)\n",
    "lr_final_accuracy = lr_final.score(tf_dev_plus, dev_labels) \n",
    "lr_final_predicted = lr_final.predict(tf_dev_plus)\n",
    "lr_final_f1 = metrics.f1_score(lr_final_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Logistic Regression\", lr_final_accuracy, lr_final_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By combing various improvments to the Logistic Regression model, we were able to predict with approximately 75% accuracy of our development dataset.  This included tuning of hyperparameters, weighting the training dataset for class imbalance, and introducing features beyond the textual vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next attempt to improve the performance of our Logistic Regression model by first training such a model with L1 regularization and using only the features in that model that are non-zero as the input for a model that uses L2 regularization. We are effectively removing unimportant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11690 features have been removed, and 84 remain.\n",
      "The accuracy of Logistic Regression model is 76.53%\n",
      "\n",
      "The F1 scores are:\n",
      "False: 84.84\n",
      "True: 35.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit an L1 model \n",
    "lr_with_l1 = LogisticRegression(penalty='l1', C=.1, tol=.01)\n",
    "lr_with_l1.fit(tf_train_plus, train_labels)\n",
    "\n",
    "# find the features whose weights have not been reduced to 0\n",
    "non_zero_feature_indices = lr_with_l1.coef_.nonzero()[1]\n",
    "\n",
    "# use only those features as the input to the L2 model\n",
    "tf_train_reduced = tf_train_plus[:, non_zero_feature_indices]\n",
    "tf_dev_reduced = tf_dev_plus[:, non_zero_feature_indices]\n",
    "\n",
    "# see how many features we've eliminated\n",
    "features_remaining = tf_train_reduced.shape[1]\n",
    "\n",
    "print '{} features have been removed, and {} remain.'.format(\n",
    "    tf_train_plus.shape[1] - features_remaining, features_remaining) \n",
    "\n",
    "# score the revised model\n",
    "lr_reduced = LogisticRegression(C=0.01, class_weight=weight_dict).fit(tf_train_reduced, train_labels)\n",
    "lr_predicted = lr_reduced.predict(tf_dev_reduced)\n",
    "lr_accuracy = lr_reduced.score(tf_dev_reduced, dev_labels)\n",
    "lr_f1_score = metrics.f1_score(lr_predicted, dev_labels, average=None)\n",
    "print_model_scores(\"Logistic Regression\", lr_accuracy, lr_final_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
